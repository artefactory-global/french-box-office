{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "broad-fancy",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://upload.wikimedia.org/wikipedia/fr/thumb/e/e5/Logo_%C3%A9cole_des_ponts_paristech.svg/676px-Logo_%C3%A9cole_des_ponts_paristech.svg.png\" width=\"200\"  height=\"200\" hspace=\"200\"/> </td>\n",
    "<td> <img src=\"https://pbs.twimg.com/profile_images/1156541928193896448/5ihYIbCQ_200x200.png\" width=\"200\" height=\"200\" /> </td>\n",
    "</tr></table>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<h1><center>Session 7 - Model Evaluation and Selection</center></h1>\n",
    "\n",
    "\n",
    "\n",
    "<font size=\"3\">This session is divided into **2** parts:\n",
    "- **Model selection**\n",
    "- **Model optimization:**\n",
    ">  * 1-Features selection\n",
    ">  * 2-Hyperparameters optimization\n",
    "\n",
    "In each of these parts, some **guidelines** and **hints** are given for each task. \n",
    "Do not hesitate to check the links to documentation to understand the functions you use. \n",
    "    \n",
    "The goal of this session is to **select a model** that you will use as your best candidate and optimize it to get the best out of it.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-office",
   "metadata": {},
   "source": [
    "# Session 5 modelling wrap up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-liberia",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics._regression import _check_reg_targets\n",
    "\n",
    "#models\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-snapshot",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    print(f\"loading raw data..\")\n",
    "    data = pd.read_csv(path)\n",
    "    return data\n",
    "\n",
    "def clean_data(data):\n",
    "    print(f\"cleaning data..\")\n",
    "    data = data.dropna()\n",
    "    data = data.query(\"year != 2020\")\n",
    "    data = data.sort_values(by='release_date')\n",
    "    data.release_date = pd.to_datetime(data.release_date)\n",
    "    data.index = data.release_date\n",
    "    data = data.drop(columns = ['index', 'release_date', 'year'], errors='ignore')\n",
    "    return data\n",
    "\n",
    "def train_test_split_by_date(df: pd.DataFrame, split_date: str):\n",
    "    \"\"\"Split dataset according to a split date in format \"YYYY-MM-DD\"\n",
    "    - train: [:split_date[\n",
    "    - test: [split_date:]\n",
    "    \"\"\"\n",
    "    train = df.loc[:split_date].copy()\n",
    "    test = df.loc[split_date:].copy()\n",
    "    return train, test\n",
    "\n",
    "def get_x_y(dataset):\n",
    "    target = dataset.sales\n",
    "    target = target.astype(float)\n",
    "    features = dataset.drop(columns = ['sales'], errors='ignore')\n",
    "    return features, target\n",
    "          \n",
    "def transform_target(target, forward = True):\n",
    "    if forward == True: target_tf = [np.log(x) for x in target]\n",
    "    else: target_tf = [np.exp(x) for x in target]\n",
    "    return target_tf  \n",
    "          \n",
    "def get_evaluation_metrics(y_test, y_pred, y_train=None) -> dict:\n",
    "    metrics = {\n",
    "        'mape': mean_absolute_percentage_error(y_test, y_pred),\n",
    "        'rmse': mean_squared_error(y_test, y_pred, squared=False),\n",
    "        'mae': mean_absolute_error(y_test, y_pred),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    \"\"\"in percent\"\"\"\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred)/y_true)) * 100\n",
    "\n",
    "def prettify_metrics(metrics: dict) -> str:\n",
    "    output = [f\"Evaluation:\\n{'-'*10}\"]\n",
    "    for name, metric in metrics.items():\n",
    "        output.append((f'- {name.upper()}: {round(metric, 2)}'))\n",
    "    return '\\n'.join(output) +'\\n'\n",
    "\n",
    "def train(lr, features, target, transformer = None):\n",
    "    print(f\"start fitting a {lr.__class__}...\")\n",
    "    if transformer:\n",
    "        lr = lr.fit(features, transformer(target, forward = True))\n",
    "    predicted_target = lr.predict(features)\n",
    "    if transformer:\n",
    "        predicted_target = transformer(predicted_target, forward= False)\n",
    "    print(prettify_metrics(get_evaluation_metrics(target, predicted_target)))\n",
    "    \n",
    "    return lr\n",
    "\n",
    "def evaluate(lr, features, target, transformer=None, ret=False):\n",
    "    predicted_target = lr.predict(features)\n",
    "    if transformer:\n",
    "        predicted_target = transformer(predicted_target, forward= False)\n",
    "    \n",
    "    print(prettify_metrics(get_evaluation_metrics(target, predicted_target)))\n",
    "    if ret==True:\n",
    "        return get_evaluation_metrics(target, predicted_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-fiber",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-hampshire",
   "metadata": {},
   "source": [
    "## Q1 - Train/Test --> Train/Validation/Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-currency",
   "metadata": {},
   "source": [
    "#### What if we want to use 2020 as testing set ?\n",
    "> - Do not drop 2020 data\n",
    "> -  Modify the function **train_test_split_date** to produce a **train_validation_test_split_date**\n",
    "> -  Hint: add another date split to the function or use the function twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-bookmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the cleaner\n",
    "def clean_data(data):\n",
    "    print(f\"cleaning data..\")\n",
    "    data = data.dropna()\n",
    "    data = data.query(\"year != 2020\")\n",
    "    data = data.sort_values(by='release_date')\n",
    "    data.release_date = pd.to_datetime(data.release_date)\n",
    "    data.index = data.release_date\n",
    "    data = data.drop(columns = ['index', 'release_date','year'], errors='ignore')\n",
    "    return data\n",
    "\n",
    "def train_test_split_by_date(df: pd.DataFrame, split_date: str ...):\n",
    "    \n",
    "    ...\n",
    "                             \n",
    "    return train, validation, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-parent",
   "metadata": {},
   "source": [
    "## Model training\n",
    "> I wrap up the training steps in a function with optional testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-organic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(learner, \n",
    "                 data, \n",
    "                 split_date_val='2018-01-01', \n",
    "                 split_date_test='2020-01-01',\n",
    "                 test = False):\n",
    "    \n",
    "    data = clean_data(data)\n",
    "    train_data, validation_data, test_data = train_test_split_by_date(data,\n",
    "                                                                      split_date_val,\n",
    "                                                                      split_date_test)\n",
    "    train_x, train_y = get_x_y(train_data)\n",
    "    validation_x, validation_y = get_x_y(validation_data)\n",
    "    test_x, test_y = get_x_y(test_data)\n",
    "    \n",
    "    lr = train(learner, train_x, train_y, transformer = transform_target)\n",
    "    print(\"Evaluate on validation set ...\")\n",
    "    evaluate(lr, validation_x, validation_y, transformer = transform_target)\n",
    "    if test == True:\n",
    "        print(\"Evaluate on test set ...\")\n",
    "        evaluate(lr, test_x, test_y, transformer = transform_target)\n",
    "    return lr, train_x.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-kuwait",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-consultation",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/yaguethiam/Ponts/data_prepared_ponts_v4.csv'\n",
    "raw_data = load_dataset(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-independence",
   "metadata": {},
   "source": [
    "# Models training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-burst",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a linear regression, a Random Forest and a LGBM on the dataset without testing\n",
    "#what metrics is more representative for our problem?\n",
    "#compare the models\n",
    "# select the best model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-light",
   "metadata": {},
   "source": [
    "#### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-hypothetical",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "relevant-uncle",
   "metadata": {},
   "source": [
    "#### Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-right",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "favorite-nerve",
   "metadata": {},
   "source": [
    "#### LGBM Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-jewelry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "social-dollar",
   "metadata": {},
   "source": [
    "## What are your thoughts?\n",
    "> - Which model is overfitting ?\n",
    "> - Which model is underfittin?\n",
    "> - Which model is the best?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-budapest",
   "metadata": {},
   "source": [
    "### Run the best model on the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-heating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the results on train/validation/test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-emerald",
   "metadata": {},
   "source": [
    "# Model Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-interference",
   "metadata": {},
   "source": [
    "## 1 - Features selection : Importance Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-quantum",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save it in a dataframe in descending order (from most import to less important)\n",
    "#plot the feature importance for the LGBM model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop on all features from 1 to n and keep track of the MAPE\n",
    "# How do you select the most optimal set of features?\n",
    "# What is the top k features given the criterion you use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot MAPE vs number of features \n",
    "#what do you observe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-islam",
   "metadata": {},
   "source": [
    "## 2 - Select the best Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-screening",
   "metadata": {},
   "source": [
    "#### Grid Search from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-accordance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-lesbian",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print lgbm parameters\n",
    "#hint place your cursor after the dot and hit tab\n",
    "lgbm_learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-uzbekistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [50, 100, 150],\n",
    "'num_leaves': [27, 31, 35]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-mixer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use ParameterGrid to print all the combinations of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-sacrifice",
   "metadata": {},
   "outputs": [],
   "source": [
    "## loop into this list of hyperparameters and get the best subset by logging MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-saskatchewan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a model with the best set of features "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-repair",
   "metadata": {},
   "source": [
    "#### Randomized search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-queen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how would you bring randomization on the search?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
