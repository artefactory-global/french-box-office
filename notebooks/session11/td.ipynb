{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://upload.wikimedia.org/wikipedia/fr/thumb/e/e5/Logo_%C3%A9cole_des_ponts_paristech.svg/676px-Logo_%C3%A9cole_des_ponts_paristech.svg.png\" width=\"200\"  height=\"200\" hspace=\"200\"/> </td>\n",
    "<td> <img src=\"https://pbs.twimg.com/profile_images/1156541928193896448/5ihYIbCQ_200x200.png\" width=\"200\" height=\"200\" /> </td>\n",
    "</tr></table>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<h1><center>Session 11 - Models lifecycle and production deployment</center></h1>\n",
    "\n",
    "\n",
    "\n",
    "<font size=\"3\">This session is divided into **2** parts:\n",
    "- **Code packaging**\n",
    "- **Testing**\n",
    "\n",
    "In each of these parts, some **guidelines** and **hints** are given for each task. \n",
    "Do not hesitate to check the links to documentation to understand the functions you use. \n",
    "    \n",
    "The goal of this session is to **package your code** you used during the development phase and to implement a few tests on the data before infering on new data\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "ROOT_DIRPATH = '/Users/yaguethiam/Ponts/french-box-office/notebooks/session11/solution'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(ROOT_DIRPATH)# set the path to where you put the folder of your package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is called a magic in jupyter notebook, it is helpful when you add code to your scripts \n",
    "#while using this notebook to test them because you wont need to restart the kernel everytime you change a function\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install loguru if you dont already have it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting loguru\n",
      "  Downloading loguru-0.5.3-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 5.4 MB/s eta 0:00:011\n",
      "\u001b[?25hInstalling collected packages: loguru\n",
      "Successfully installed loguru-0.5.3\n"
     ]
    }
   ],
   "source": [
    "!pip install loguru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from loguru import logger\n",
    "\n",
    "#preprocessing\n",
    "def clean_data(data, drop_2020=True):\n",
    "    print(\"cleaning data..\")\n",
    "    data = data.dropna()\n",
    "    data.drop(['title'], axis = 1, inplace = True)\n",
    "    if drop_2020:\n",
    "        data = data.query(\"year != 2020\")\n",
    "    data = data.sort_values(by='release_date')\n",
    "    data.release_date = pd.to_datetime(data.release_date)\n",
    "    data.index = data.release_date\n",
    "    data = data.drop(columns=['index', 'release_date', 'year'], errors='ignore')\n",
    "    return data\n",
    "\n",
    "\n",
    "def train_test_split_by_date(df: pd.DataFrame, split_date_val: str, split_date_test: str):\n",
    "    \"\"\"Split dataset according to a split date in format \"YYYY-MM-DD\"\n",
    "    - train: [:split_date_1[\n",
    "    - validation: [split_date_1: split_date_2[\n",
    "    - test: [split_date_2:[\n",
    "    \"\"\"\n",
    "    train = df.loc[:split_date_val].copy()\n",
    "    validation = df.loc[split_date_val:split_date_test].copy()\n",
    "    test = df.loc[split_date_test:].copy()\n",
    "    return train, validation, test\n",
    "\n",
    "\n",
    "def get_x_y(dataset):\n",
    "    target = dataset.sales\n",
    "    target = target.astype(float)\n",
    "    features = dataset.drop(columns = ['sales'], errors='ignore')\n",
    "    return features, target\n",
    "\n",
    "\n",
    "def transform_target(target, forward = True):\n",
    "    if forward == True: target_tf = [np.log(x) for x in target]\n",
    "    else: target_tf = [np.exp(x) for x in target]\n",
    "    return target_tf\n",
    "\n",
    "#training\n",
    "def train(lr, features, target, transformer = None):\n",
    "    print(f\"start fitting a {lr.__class__}...\")\n",
    "    if transformer:\n",
    "        lr = lr.fit(features, transformer(target, forward = True))\n",
    "    predicted_target = lr.predict(features)\n",
    "    if transformer:\n",
    "        predicted_target = transformer(predicted_target, forward= False)\n",
    "    print(get_evaluation_metrics(target, predicted_target))\n",
    "    return lr\n",
    "\n",
    "\n",
    "def predict(model, features, transformer=None):\n",
    "    print(\"Predicting on new data..\")\n",
    "    predicted_target = model.predict(features)\n",
    "    if transformer:\n",
    "        predicted_target = transformer(predicted_target, forward=False)\n",
    "    return predicted_target\n",
    "\n",
    "\n",
    "def save_model(model: LGBMRegressor, filepath: str):\n",
    "    model.booster_.save_model(filepath, num_iteration=model.best_iteration_)\n",
    "    print(f'Model saved to {filepath}')\n",
    "\n",
    "\n",
    "#evaluate\n",
    "def get_evaluation_metrics(y_test, y_pred, y_train=None) -> dict:\n",
    "    metrics = {\n",
    "        'mape': mean_absolute_percentage_error(y_test, y_pred),\n",
    "        'rmse': mean_squared_error(y_test, y_pred, squared=False),\n",
    "        'mae': mean_absolute_error(y_test, y_pred),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    \"\"\"in percent\"\"\"\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred)/y_true)) * 100\n",
    "\n",
    "\n",
    "def prettify_metrics(metrics: dict) -> str:\n",
    "    output = [f\"Evaluation:\\n{'-'*10}\"]\n",
    "    for name, metric in metrics.items():\n",
    "        output.append((f'- {name.upper()}: {round(metric, 2)}'))\n",
    "    return '\\n'.join(output) +'\\n'\n",
    "\n",
    "\n",
    "def evaluate(lr, features, target, transformer=None, ret=False):\n",
    "    predicted_target = lr.predict(features)\n",
    "    if transformer:\n",
    "        predicted_target = transformer(predicted_target, forward=False)\n",
    "    \n",
    "    print(get_evaluation_metrics(target, predicted_target))\n",
    "    if ret==True:\n",
    "        return get_evaluation_metrics(target, predicted_target)\n",
    "\n",
    "#utils\n",
    "def load_dataset(path: str) -> pd.DataFrame:\n",
    "    print(f\"loading raw data {path}...\")\n",
    "    data = pd.read_csv(path)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_features_list(data):\n",
    "    print(f\"getting features list...\")\n",
    "    return set(data.columns)\n",
    "\n",
    "\n",
    "def save_results(preditions_df, path):\n",
    "    print(f\"Saving predictions in {path} ...\")\n",
    "    preditions_df.to_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code packaging: the training workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this part is to build a training flow that is easy to run when needed. It should be like:\n",
    "\n",
    "`from bin.train import training_workflow\n",
    "training_workflow()`\n",
    "\n",
    "The functions above are all you need to build the training workflow. We have provided you a folder containing a structure for you to package your application in a way you can make the call above.\n",
    "\n",
    "Copy and paste the functions where you think they should be and import them where you need them.\n",
    "\n",
    "- TIPS:\n",
    "When you package your code, use a logger instead of print to show comments. \n",
    "Example:\n",
    "\n",
    "`from loguru import logger\n",
    "logger.info(\"print something\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATASET_FILEPATH = os.path.join(ROOT_DIRPATH, 'data', \"processed_dataset.csv\")\n",
    "LGBM_MODEL_FILEPATH = os.path.join(ROOT_DIRPATH, \"models\", \"light_gbm_model.txt\")\n",
    "START_VALIDATION_DATE = '2018-01-01'\n",
    "START_TEST_DATE = '2020-01-01'\n",
    "\n",
    "FEATURE_IMPORTANCE = [\n",
    "    \"runtime\",\n",
    "    \"mean_5_popularity\",\n",
    "    \"mean_3_popularity\",\n",
    "    \"budget\",\n",
    "    \"actor_1_sales\",\n",
    "    \"mean_sales_actor\",\n",
    "    \"max_sales_actor\",\n",
    "    \"actor_3_sales\",\n",
    "    \"actor_2_sales\",\n",
    "    \"month\",\n",
    "    \"cos_month\",\n",
    "    \"Comédie\",\n",
    "    \"Drame\",\n",
    "    \"is_part_of_collection\",\n",
    "    \"rolling_sales_collection\",\n",
    "    \"prod_FR\",\n",
    "    \"Action\",\n",
    "    \"prod_OTHER\",\n",
    "    \"available_lang_fr\",\n",
    "    \"original_lang_fr\",\n",
    "    \"holiday\",\n",
    "    \"Romance\",\n",
    "    \"original_lang_en\",\n",
    "    \"prod_US\",\n",
    "    \"Familial\",\n",
    "    \"nb_movie_collection\",\n",
    "    \"Horreur\",\n",
    "    \"available_lang_other\",\n",
    "    \"prod_GB\",\n",
    "    \"Other\",\n",
    "    \"original_lang_other\",\n",
    "    \"available_lang_it\",\n",
    "    \"Fantastique\",\n",
    "    \"available_lang_en\",\n",
    "    \"vacances_zone_c\",\n",
    "    \"vacances_zone_a\",\n",
    "    \"available_lang_es\",\n",
    "    \"vacances_zone_b\",\n",
    "    \"prod_BE\",\n",
    "    \"available_lang_de\",\n",
    "    \"original_lang_ja\",\n",
    "    \"prod_CA\",\n",
    "    \"original_lang_it\",\n",
    "    \"prod_DE\",\n",
    "    \"available_lang_ja\",\n",
    "    \"jour_ferie\",\n",
    "    \"original_lang_es\",\n",
    "]\n",
    "BEST_K_FEATURES = 36  # K best features sorted by feature importance\n",
    "\n",
    "# LightGBM hyperparameters\n",
    "LGBM_BEST_PARAMS = {\n",
    "    \"max_depth\": 70,\n",
    "    \"n_estimators\": 80,\n",
    "    \"num_leaves\": 31,\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading raw data /Users/yaguethiam/Ponts/french-box-office/notebooks/session11/solution/data/processed_dataset.csv...\n",
      "cleaning data..\n"
     ]
    }
   ],
   "source": [
    "#load data and clean dataset\n",
    "raw_data = load_dataset(TRAINING_DATASET_FILEPATH)\n",
    "data = clean_data(raw_data, drop_2020=False)\n",
    "train_data, validation_data, _ = train_test_split_by_date(data,\n",
    "                                                          START_VALIDATION_DATE,\n",
    "                                                          START_TEST_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get train and validation data and select only the best k features we selected during dev phase\n",
    "train_x, train_y = get_x_y(train_data)\n",
    "validation_x, validation_y = get_x_y(validation_data)\n",
    "\n",
    "train_x = train_x[FEATURE_IMPORTANCE[:BEST_K_FEATURES]]\n",
    "validation_x = validation_x[FEATURE_IMPORTANCE[:BEST_K_FEATURES]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LightGBM using hyper-parameters: {'max_depth': 70, 'n_estimators': 80, 'num_leaves': 31}\n",
      "start fitting a <class 'lightgbm.sklearn.LGBMRegressor'>...\n",
      "{'mape': 187.99827186360915, 'rmse': 234171.0957984909, 'mae': 102878.43197478513}\n"
     ]
    }
   ],
   "source": [
    "#run a LGBM regressor\n",
    "lgbm = LGBMRegressor(**LGBM_BEST_PARAMS)\n",
    "print(f\"Training LightGBM using hyper-parameters: {LGBM_BEST_PARAMS}\")\n",
    "lgbm = train(lgbm, train_x, train_y, transformer=transform_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on validation set ...\n",
      "{'mape': 376.8063413834246, 'rmse': 238484.95219472746, 'mae': 103569.43339518228}\n"
     ]
    }
   ],
   "source": [
    "#validate\n",
    "print(\"Evaluate on validation set ...\")\n",
    "evaluate(lgbm, validation_x, validation_y, transformer=transform_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to /Users/yaguethiam/Ponts/french-box-office/notebooks/session11/solution/models/light_gbm_model.txt\n"
     ]
    }
   ],
   "source": [
    "#save the model\n",
    "save_model(lgbm, LGBM_MODEL_FILEPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this part is to write simple tests for the input data in live mode\n",
    "- 1: build a test to check whether all the features expected are present in the input data\n",
    "- 2: build a test to check whether the column runtime only contain positive number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of a test\n",
    "\n",
    "`def test_sum():\n",
    "    assert sum([1, 2, 3]) == 6, \"Should be 6\"`\n",
    "\n",
    "- `assert` : python keyword to check an affirmation\n",
    "- `sum([1, 2, 3]) == 6` is what you want to check\n",
    "- `\"Should be 6\"` : a message to display only when the test fails\n",
    "\n",
    "You can also give a input data to your test function( example give as input the data you want to test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have provided 3 datasets for inference. Run the tests on these dataset and comment your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run tests on infer_data_1.csv, infer_data_2.csv and infer_data_1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infer on new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this part is to use the model and infer on new data. \n",
    "From the tests above, select a suitable data and run the model on it. Then save the results in a csv file with the title and the sales predicted.\n",
    "Running the inference workflow should be as easy as :\n",
    "\n",
    "`from bin.inference import inference_workflow\n",
    "inference_workflow(str(Path(ROOT_PATH,'data/test_data/test_valid.csv')))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = Path(ROOT_DIRPATH, 'data/test_data/infer_data_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading raw data /Users/yaguethiam/Ponts/french-box-office/notebooks/session11/solution/data/test_data/infer_data_3.csv...\n",
      "cleaning data..\n"
     ]
    }
   ],
   "source": [
    "raw_data = load_dataset(path_data)\n",
    "data = clean_data(raw_data, drop_2020=False)\n",
    "test_x, _ = get_x_y(data)\n",
    "test_x = test_x[FEATURE_IMPORTANCE[:BEST_K_FEATURES]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on new data..\n"
     ]
    }
   ],
   "source": [
    "#load model\n",
    "model = lgb.Booster(model_file= str(Path(ROOT_DIRPATH, 'models/light_gbm_model.txt')))\n",
    "predicted_target = predict(model, test_x, transformer=transform_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[82799.60235992783, 102150.97954882764, 19370.508751992387, 62662.726585521865, 229001.89858818284, 40805.87188563293, 32789.463494470954, 140873.31504568606, 32720.16701623868, 233246.48634825862]\n",
      "Saving predictions in /Users/yaguethiam/Ponts/french-box-office/notebooks/session11/solution/data/test_data/prediction_infer_data_3.csv ...\n"
     ]
    }
   ],
   "source": [
    "#save results\n",
    "raw_data['predicted_sales'] = predicted_target\n",
    "print(predicted_target[:10])\n",
    "save_results(raw_data[['title', 'predicted_sales']], Path(Path(path_data).parent,'prediction_'+Path(path_data).name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
