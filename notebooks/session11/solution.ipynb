{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://upload.wikimedia.org/wikipedia/fr/thumb/e/e5/Logo_%C3%A9cole_des_ponts_paristech.svg/676px-Logo_%C3%A9cole_des_ponts_paristech.svg.png\" width=\"200\"  height=\"200\" hspace=\"200\"/> </td>\n",
    "<td> <img src=\"https://pbs.twimg.com/profile_images/1156541928193896448/5ihYIbCQ_200x200.png\" width=\"200\" height=\"200\" /> </td>\n",
    "</tr></table>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<h1><center>Session 11 - Model Serving</center></h1>\n",
    "\n",
    "\n",
    "\n",
    "<font size=\"3\">This session is divided into **2** parts:\n",
    "- **Model selection**\n",
    "- **Model optimization:**\n",
    ">  * 1-Hyperparameters optimization\n",
    ">  * 2-Features selection\n",
    "\n",
    "In each of these parts, some **guidelines** and **hints** are given for each task. \n",
    "Do not hesitate to check the links to documentation to understand the functions you use. \n",
    "    \n",
    "The goal of this session is to **select a model** that you will use as your best candidate and optimize it to get the best out of it.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from lib.evaluation.evaluate import evaluate\n",
    "from lib.modelling.training import train\n",
    "from lib.preprocessing.preprocess import (clean_data, get_x_y,\n",
    "                                          train_test_split_by_date,\n",
    "                                          transform_target)\n",
    "from lib.utils.io import load_dataset\n",
    "from lightgbm import LGBMRegressor\n",
    "from loguru import logger\n",
    "from config import TRAINING_DATASET_FILEPATH, LGBM_BEST_PARAMS, BEST_K_FEATURES, FEATURE_IMPORTANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-23 16:06:10.320 | INFO     | lib.utils.io:load_dataset:27 - loading raw data /Users/hugo/Documents/PONTS/french-box-office/data/data_prepared_session4.csv...\n",
      "2021-04-23 16:06:10.455 | INFO     | lib.preprocessing.preprocess:clean_data:7 - cleaning data..\n",
      "2021-04-23 16:06:10.511 | INFO     | __main__:<module>:13 - Training fitting LightGBM using features: ['runtime', 'mean_5_popularity', 'mean_3_popularity', 'budget', 'actor_1_sales', 'mean_sales_actor', 'max_sales_actor', 'actor_3_sales', 'actor_2_sales', 'month', 'cos_month', 'Com√©die', 'Drame', 'is_part_of_collection', 'rolling_sales_collection', 'prod_FR', 'Action', 'prod_OTHER', 'available_lang_fr', 'original_lang_fr', 'holiday', 'Romance', 'original_lang_en', 'prod_US', 'Familial', 'nb_movie_collection', 'Horreur', 'available_lang_other', 'prod_GB', 'Other', 'original_lang_other', 'available_lang_it', 'Fantastique', 'available_lang_en', 'vacances_zone_c', 'vacances_zone_a', 'available_lang_es']hyper-parameters: {'max_depth': 70, 'n_estimators': 80, 'num_leaves': 31}\n",
      "2021-04-23 16:06:10.514 | INFO     | lib.modelling.training:train:6 - start fitting a <class 'lightgbm.sklearn.LGBMRegressor'>...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-23 16:06:10.780 | INFO     | lib.modelling.training:train:12 - {'mape': 192.63352372800645, 'rmse': 234053.95434579128, 'mae': 102825.81323968484}\n",
      "2021-04-23 16:06:10.783 | INFO     | __main__:<module>:15 - Evaluate on validation set ...\n",
      "2021-04-23 16:06:10.801 | INFO     | lib.evaluation.evaluate:evaluate:33 - {'mape': 364.0570041903621, 'rmse': 238342.93099787852, 'mae': 104460.42007271445}\n",
      "2021-04-23 16:06:10.802 | INFO     | __main__:<module>:17 - Evaluate on test set...\n",
      "2021-04-23 16:06:10.824 | INFO     | lib.evaluation.evaluate:evaluate:33 - {'mape': 177.9903019586095, 'rmse': 115865.58894202021, 'mae': 63725.9358367287}\n"
     ]
    }
   ],
   "source": [
    "raw_data = load_dataset(TRAINING_DATASET_FILEPATH)\n",
    "data = clean_data(raw_data, drop_2020=False)\n",
    "train_data, validation_data, test_data = train_test_split_by_date(data,\n",
    "                                                                '2018-01-01',\n",
    "                                                                '2020-01-01')\n",
    "train_x, train_y = get_x_y(train_data)\n",
    "validation_x, validation_y = get_x_y(validation_data)\n",
    "test_x, test_y = get_x_y(test_data)\n",
    "lgbm = LGBMRegressor(**LGBM_BEST_PARAMS)\n",
    "features_list = FEATURE_IMPORTANCE[:BEST_K_FEATURES+1]\n",
    "msg = f\"Training fitting LightGBM using features: {features_list}\"\n",
    "msg += f\"hyper-parameters: {LGBM_BEST_PARAMS}\"\n",
    "logger.info(msg)\n",
    "lgbm = train(lgbm, train_x[features_list], train_y, transformer=transform_target)\n",
    "logger.info(\"Evaluate on validation set ...\")\n",
    "evaluate(lgbm, validation_x[features_list], validation_y, transformer=transform_target)\n",
    "logger.info(\"Evaluate on test set...\")\n",
    "evaluate(lgbm, test_x[features_list], test_y, transformer=transform_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lightgbm.sklearn.LGBMRegressor"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x10fcea750>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find how to save the model in a file\n",
    "lgbm.booster_.save_model('model.txt', num_iteration=lgbm.best_iteration_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, filepath):\n",
    "    model.booster_.save_model(filepath, num_iteration=lgbm.best_iteration_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from the file\n",
    "model = LGBMRegressor(model_file='model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-23 16:13:34.567 | INFO     | lib.evaluation.evaluate:evaluate:33 - {'mape': 177.9903019586095, 'rmse': 115865.58894202021, 'mae': 63725.9358367287}\n"
     ]
    }
   ],
   "source": [
    "evaluate(lgbm, test_x[features_list], test_y, transformer=transform_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, features, transformer=None, ret=False):\n",
    "    predicted_target = model.predict(features)\n",
    "    if transformer:\n",
    "        predicted_target = transformer(predicted_target, forward=False)\n",
    "    return predicted_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_json = test_x.iloc[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_foo = pd.DataFrame([test_x.iloc[0].to_dict()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(lgbm, test_foo[features_list], transformer=transform_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17607.51362607005]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make a requests again server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.post('http://0.0.0.0:8080/predict', json={'user': 'john'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\"hello john\"'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply on new movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'read_movies_entrees' from 'lib.utils.io' (/Users/hugo/Documents/PONTS/french-box-office/lib/utils/io.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-39cd5c632e33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_movies_entrees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_movies_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_boxoffice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_movies_entrees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROOT_DIRPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'french-box-office-23april2021.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_movies_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROOT_DIRPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'movie-features-23-april-2021.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'read_movies_entrees' from 'lib.utils.io' (/Users/hugo/Documents/PONTS/french-box-office/lib/utils/io.py)"
     ]
    }
   ],
   "source": [
    "from lib.utils.io import read_movies_entrees, read_movies_features\n",
    "\n",
    "df_boxoffice = read_movies_entrees(os.path.join(ROOT_DIRPATH, 'data', 'french-box-office-23april2021.json'))\n",
    "df_features = read_movies_features(os.path.join(ROOT_DIRPATH, 'data', 'movie-features-23-april-2021.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vacances_scolaires_france import SchoolHolidayDates\n",
    "\n",
    "fr_holidays = SchoolHolidayDates()\n",
    "df_vacances = pd.DataFrame()\n",
    "for year in [2021]:\n",
    "    df_vacances = pd.concat([df_vacances, pd.DataFrame.from_dict(fr_holidays.holidays_for_year(year)).T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_film = json.loads(NEW_FILM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We need to preprocess data - Package the following code into one workflow function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are some functions we wrote to help you, feel free to check them out to see what they do\n",
    "lang_to_keep = ['en', 'fr', 'es', 'it', 'ja', 'de']\n",
    "country_to_keep = ['FR', 'US', 'GB', 'DE', 'BE', 'CA']\n",
    "dict_genres = {\n",
    "    'Drame': 'Drame',\n",
    "    'Com√©die': 'Com√©die',\n",
    "    'Romance': 'Romance',\n",
    "    'Action': 'Action',\n",
    "    'Thriller': 'Action',\n",
    "    'Aventure': 'Action',\n",
    "    'Crime': 'Action',\n",
    "    'Guerre': 'Action',\n",
    "    'Western': 'Action',\n",
    "    'Familial': 'Familial',\n",
    "    'Animation': 'Familial',\n",
    "    'Fantastique': 'Fantastique',\n",
    "    'Science-Fiction': 'Fantastique',\n",
    "    'Horreur': 'Horreur',\n",
    "    'Myst√®re': 'Other',\n",
    "    'Musique': 'Other',\n",
    "    'Histoire': 'Other',\n",
    "    'Documentaire': 'Other',\n",
    "    'T√©l√©film': 'Other'\n",
    "}\n",
    "\n",
    "\n",
    "def read_movies_entrees(path):\n",
    "    '''\n",
    "    Read the box office dataset \n",
    "    and casts it as an usable pandas DataFrame\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path: str\n",
    "        path to the dataset\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df: pd.DataFrame\n",
    "        Data as DataFrame\n",
    "    '''\n",
    "    bo = read_from_json(path)\n",
    "    bo = [\n",
    "        {\n",
    "            \"year\": item['year'], \n",
    "            \"title\": item['title'], \n",
    "            \"id\": int(item['id']), \n",
    "            \"sales\": item['first_week_sales'],\n",
    "            \"release_date\": item['release_date']\n",
    "        } for item in bo\n",
    "    ]\n",
    "    return pd.DataFrame(bo)\n",
    "\n",
    "\n",
    "def read_movies_features(path):\n",
    "    '''\n",
    "    Read the movie features dataset \n",
    "    and casts it as an usable pandas DataFrame\n",
    "    N.B: Fields that are not yet used are commented\n",
    "    Parameters\n",
    "    ----------\n",
    "    path: str\n",
    "        path to the dataset\n",
    "    Returns\n",
    "    -------\n",
    "    df: pd.DataFrame\n",
    "        Data as DataFrame\n",
    "    '''\n",
    "    features = read_from_json(path)\n",
    "    features = [\n",
    "        {\n",
    "            \"is_adult\": item['adult'],\n",
    "            \"is_part_of_collection\": not not item['belongs_to_collection'],\n",
    "            \"collection_name\": item['belongs_to_collection']['name'] if item['belongs_to_collection'] != {} else None, # Currently simple bool, may be interesting to use a more complex feature later\n",
    "            \"budget\": item['budget'],\n",
    "            \"genres\": [ genre['name'] for genre in item['genres'] ], \n",
    "            \"original_language\": item['original_language'],\n",
    "            \"overview\": item['overview'], # Not used yet. Blob of text\n",
    "            \"production_countries\": [ country['iso_code'] for country in item['production_countries'] ],\n",
    "            \"languages\": [ language['iso_code'] for language in item['languages'] ],\n",
    "            \"tagline\": item['tagline'], # Not used yet. Blob of text\n",
    "            \"runtime\": item['runtime'],\n",
    "            \"cast\": item['cast'], # Not used yet. List of dicts with actor gender, name, id...\n",
    "            \"id\": int(item['id'])\n",
    "        } for item in features\n",
    "    ]\n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "\n",
    "def read_from_json(path):\n",
    "    '''\n",
    "    Read and cast a json into a python object\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path: str\n",
    "        Path to json file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data: Union[dict, list]\n",
    "        Json casted as python object\n",
    "    '''\n",
    "    data = json.load(open(path, 'r', encoding='utf-8', errors=\"ignore\"))\n",
    "    return data\n",
    "\n",
    "\n",
    "def reduce_lang_categories(lang_list, lang_to_keep=lang_to_keep):\n",
    "    return list(set([el if el in lang_to_keep else 'other' for el in lang_list]))\n",
    "\n",
    "\n",
    "def reduce_country_categories(country_list, country_to_keep=country_to_keep):\n",
    "    return list(set([el if el in country_to_keep else 'OTHER' for el in country_list]))\n",
    "\n",
    "\n",
    "def reduce_genre_categories(genre_list, dict_genres=dict_genres):\n",
    "    return list(set([dict_genres[el] for el in genre_list]))\n",
    "\n",
    "\n",
    "def flatten_list_series(column):\n",
    "    flattened_series = column.apply(pd.Series).stack().reset_index(drop=True)\n",
    "    flattened_series.name = column.name\n",
    "    return pd.DataFrame(flattened_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_boxoffice = read_movies_entrees('../../data/french-box-office-29nov2020.json')\n",
    "df_features = read_movies_features('../../data/movie-features-29nov2020.json')\n",
    "\n",
    "data = pd.merge(df_boxoffice, df_features, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[data['sales'] != 0]\n",
    "# Missing/zero values for budget\n",
    "# Hint: check the number of missing values to see which approach is the best suited\n",
    "median = np.median(data.loc[data['budget'] != 0]['budget'])\n",
    "print(median) # 25000000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting Null and zeros values to median\n",
    "data.loc[(data['budget'] == 0) | (data['budget'].isnull()), 'budget'] = median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing/zero values for runtime\n",
    "# Hint: check the number of missing values to see which approach is the best suited\n",
    "runtime_mean = np.mean(data.loc[(data['runtime'] != 0) & (data['runtime'].isnull() == False)]['runtime'])\n",
    "print(runtime_mean)\n",
    "data.loc[(data['runtime'].isnull() == True) | (data['runtime'] == 0), 'runtime'] = runtime_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To go further:\n",
    "# Missing values for production countries\n",
    "# Hint: you can use the column \"original_language\" (e.g.: if original_language is \"FR\", the production country\n",
    "# is likely to be France)\n",
    "\n",
    "# For all movies where the production country is empty and the original language is french, we suppose that the\n",
    "# production country is France\n",
    "data.loc[(data.astype(str)['production_countries'] == '[]') & (\n",
    "    data['original_language'] == 'fr'), 'production_countries'] = ['FR']\n",
    "\n",
    "# For all movies where the production country is empty and the original language is english, we suppose that the\n",
    "# production country is USA\n",
    "data.loc[(data.astype(str)['production_countries'] == '[]') & (\n",
    "    data['original_language'] == 'en'), 'production_countries'] = ['US']\n",
    "\n",
    "# Elsewhere we put 'Other'\n",
    "data.loc[(data.astype(str)['production_countries'] == '[]'), 'production_countries'] = ['OTHER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce number of categories for: Original language\n",
    "# Hint: you can use the lang_to_keep variable from part 0, already imported. If the original language is in\n",
    "# lang_to_keep then we keep it, otherwise we set the value to 'other'\n",
    "data['original_language'] = data['original_language'].map(lambda x: x if x in lang_to_keep else 'other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce number of categories for: languages, production_countries and genres\n",
    "# For example for languages:\n",
    "data['languages'] = data['languages'].map(lambda x: reduce_lang_categories(x))\n",
    "\n",
    "# Hint: for production_countries, you can use the reduce_country_categories() function from part 0,\n",
    "# already imported\n",
    "data['production_countries'] = data['production_countries'].map(lambda x: reduce_country_categories(x))\n",
    "\n",
    "# Hint: for production_countries, you can use the reduce_genre_categories() function from part 0,\n",
    "# already imported\n",
    "data['genres'] = data['genres'].map(lambda x: reduce_genre_categories(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode is_part_of_collection into numerical\n",
    "# Hint: you can use a dictionary to map numerical values for each value of is_part_of_collection (True or False)\n",
    "dict_collection = {\n",
    "    True: 1,\n",
    "    False: 0\n",
    "}\n",
    "\n",
    "data['is_part_of_collection'] = data['is_part_of_collection'].map(dict_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode original_language\n",
    "# Hint: you can perform one-hot encoding with pd.get_dummies(), check documentation to see how to use it (\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html)\n",
    "\n",
    "# Adding the argument \"drop_first=True\" allows to to get k-1 dummies out of k categorical levels by removing the \n",
    "# first level to avoid multicolinearity\n",
    "data_final = pd.get_dummies(data, prefix='original_lang', columns=['original_language'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features with multiple categories\n",
    "# Hint: you can use MultiLabelBinarizer(). MultiLabelBinarizer is used when an observation can have multiple \n",
    "# categories: here a movie can be available in french, english, german and japanese.\n",
    "\n",
    "# Set index to ID for MultiLabelBinarizer\n",
    "data_final = data_final.set_index('id')\n",
    "\n",
    "# Languages\n",
    "mlb = MultiLabelBinarizer()\n",
    "# Create a new dataset 'df_lang' that will contain one binary column for each language\n",
    "# (ex: available_lang_fr takes 1 if 'fr' was in the column 'languages', else 0)\n",
    "df_lang = pd.DataFrame(mlb.fit_transform(data_final['languages']), columns=mlb.classes_, index=data_final.index)\n",
    "df_lang.columns = ['available_lang_' + col for col in df_lang.columns]\n",
    "\n",
    "# Genres\n",
    "mlb = MultiLabelBinarizer()\n",
    "df_genre = pd.DataFrame(mlb.fit_transform(data_final['genres']), columns=mlb.classes_, index=data_final.index)\n",
    "\n",
    "# Production countries\n",
    "mlb = MultiLabelBinarizer()\n",
    "df_country = pd.DataFrame(mlb.fit_transform(data_final['production_countries']), columns=mlb.classes_,\n",
    "                          index=data_final.index)\n",
    "df_country.columns = ['prod_' + col for col in df_country.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge encoded dataframes and store it into a new dataframe named data_final\n",
    "# Hint: you can use the merge function from pandas, pd.merge(), check documentation to see how to use it\n",
    "\n",
    "# We successively merge the three new datasets with data_final: df_lang, df_genre, df_country\n",
    "# NB: the merge function from pandas can be used in two ways: either pd.merge(df1, df2) or df1.merge(df2)\n",
    "data_final = pd.merge(data_final, df_lang, left_index=True, right_index=True) \\\n",
    "               .merge(df_genre, left_index=True, right_index=True) \\\n",
    "               .merge(df_country, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load school holidays for France\n",
    "fr_holidays = SchoolHolidayDates()\n",
    "df_vacances = pd.DataFrame()\n",
    "for year in list(set(data_final['year'])):\n",
    "    df_vacances = pd.concat([df_vacances, pd.DataFrame.from_dict(fr_holidays.holidays_for_year(year)).T])\n",
    "\n",
    "# Load bank holidays for France\n",
    "df_jf = pd.DataFrame()\n",
    "for year in list(set(data_final['year'])):\n",
    "    df_jf = pd.concat([df_jf, pd.DataFrame([\n",
    "        {'date': el[0], 'jour_ferie': el[1]} for el in sorted(holidays.FRA(years=year).items())])])\n",
    "    \n",
    "# Merge school and bank holidays\n",
    "df_holidays = pd.merge(df_vacances, df_jf, how='outer', on='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features from df_holidays dataframes (school holidays and bank holidays):\n",
    "# - 3 binary features for school holidays, taking 1 if the given zone is on holiday, else 0 (vacances_zone_a, \n",
    "# vacances_zone_b, vacances_zone_c)\n",
    "\n",
    "# Definition of a dictionary to encode boolean into numeric\n",
    "dict_map_vac = {\n",
    "    True: 1,\n",
    "    False: 0\n",
    "}\n",
    "# Apply dictionary to each holiday column for the three zones (A, B, C)\n",
    "df_holidays['vacances_zone_a'] = df_holidays['vacances_zone_a'].map(dict_map_vac)\n",
    "df_holidays['vacances_zone_b'] = df_holidays['vacances_zone_b'].map(dict_map_vac)\n",
    "df_holidays['vacances_zone_c'] = df_holidays['vacances_zone_c'].map(dict_map_vac)\n",
    "\n",
    "# - 1 binary feature for bank holiday, taking 1 if it is a bank holiday, else 0\n",
    "# The column \"jour ferie\" contains either the name of the holiday or a missing value (NaN)\n",
    "# The idea is to put a '1' when it's a holiday (i.e. when the value is different from nan, else 0)\n",
    "df_holidays['jour_ferie'] = df_holidays['jour_ferie'].map(lambda x: 1 if str(x) != 'nan' else 0)\n",
    "\n",
    "# - To go further: Try to create a combined feature with school and bank holidays\n",
    "df_holidays['holiday'] = df_holidays['vacances_zone_a'] + df_holidays['vacances_zone_b'] + df_holidays[\n",
    "    'vacances_zone_c'] + df_holidays['jour_ferie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_holidays that contains newly created features with the main dataframe and store it into data_final\n",
    "\n",
    "# We change the type of the column \"date\" of the dataframe df_holidays to be able to merge it with our main\n",
    "# dataframe. Otherwise we would get a Type Error because you cannot merge on a column that has different types\n",
    "df_holidays['date'] = df_holidays['date'].map(lambda x: str(x))\n",
    "data_final_cal = pd.merge(data_final, df_holidays, how='left', left_on='release_date', right_on='date').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_final_cal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-a0ca9315345e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# NB: when you use \"[\" and \"]\", the last number is not included, therefore x[5:7] will take characters from the 5th\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# position to the 7th position not included (i.e. the 5th and 6th characters)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdata_final_cal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'month'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_final_cal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'release_date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# To go further: try to transform the \"month\" variable using a mathematical function to capture cyclicity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_final_cal' is not defined"
     ]
    }
   ],
   "source": [
    "# Create calendar features for month: \n",
    "# - the number of the month (named \"month\")\n",
    "\n",
    "# The date is in this format : \"2019-02-06\", the month corresponds to the 5th and 6th characters so we extract it\n",
    "# NB: when you use \"[\" and \"]\", the last number is not included, therefore x[5:7] will take characters from the 5th\n",
    "# position to the 7th position not included (i.e. the 5th and 6th characters)\n",
    "data_final_cal['month'] = data_final_cal['release_date'].map(lambda x: int(x[5:7]))\n",
    "\n",
    "# To go further: try to transform the \"month\" variable using a mathematical function to capture cyclicity\n",
    "# (January (1) comes right after December (12))\n",
    "\n",
    "def apply_cos(df: pd.DataFrame,\n",
    "              x: str, col_name: str, period: int) -> pd.DataFrame:\n",
    "    \"\"\" Cos function on a column, for a specified period\n",
    "    \"\"\"\n",
    "    df[col_name] = 2 * np.cos(2 * np.pi * df[x] / period)\n",
    "    return df\n",
    "\n",
    "data_final_cal = apply_cos(data_final_cal, 'month', 'cos_month', 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collection with an high number of movies are often sagas that have worked well (ex: Star Wars, Fast and\n",
    "# Furious, ...)\n",
    "# We can therefore use the variable \"is_part_of_collection\" to compute the number of movies per collection\n",
    "# Hint: to create this kind of feature, you can use the .value_counts() method\n",
    "\n",
    "# Exclude collections with only one movie\n",
    "\n",
    "# We count the number of movies per collection\n",
    "df_count_col = data_final_cal.groupby(['collection_name']).count().reset_index()\n",
    "# We define a list of collection names with less than 2 movies (we won't take them into account)\n",
    "not_collection = list(set(df_count_col.loc[df_count_col['year'] < 2]['collection_name']))\n",
    "# For movies with less than 2 movies per collection, we set the values of \"is_part_of_colleciton\" to 0\n",
    "data_final_cal.loc[data_final_cal['collection_name'].isin(not_collection), 'is_part_of_collection'] = 0\n",
    "# For movies with less than 2 movies per collection, we set the values of \"collection_name\" to None\n",
    "data_final_cal.loc[data_final_cal['collection_name'].isin(not_collection), 'collection_name'] = None\n",
    "\n",
    "# Create the feature: number of movies per collection\n",
    "# We define a dictionary with the number of movies per collection (only collections with at least 2 movies since\n",
    "# we excluded the other ones just before)\n",
    "map_col_count = dict(data_final_cal['collection_name'].value_counts())\n",
    "# We remove the \"None\" collection (i.e. the first item of the dictionary that corresponds to all movies that are\n",
    "# not part of a collection)\n",
    "del map_col_count[0]\n",
    "# We map the dictionary into a new feature: the number of movies per collection\n",
    "data_final_cal['nb_movie_collection'] = data_final_cal['collection_name'].map(map_col_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To go (much) further:\n",
    "# Movies from a same collection will tend to have a similar number of sales\n",
    "# We can therefore use the variable \"is_part_of_collection\" to calculate an average of the sales of previous films\n",
    "# from the same collection\n",
    "# Hint: to create this kind of feature, you can use the .groupby(), .transform(), .rolling(), .mean() and .shift()\n",
    "# methods\n",
    "\n",
    "# We isolate the movies that are part of a collection and we store it into df_collection\n",
    "df_collection = data_final_cal.loc[data_final_cal['is_part_of_collection'] == 1]\n",
    "# We compute the rolling average of the sales of the 10 previous movies per collection \n",
    "df_collection['rolling_sales_collection'] = df_collection.sort_values(by=['collection_name', 'release_date']) \\\n",
    "             .groupby('collection_name')['sales'] \\\n",
    "             .transform(lambda x: x.rolling(10, 1).mean().shift())\n",
    "\n",
    "\n",
    "# Merge with main dataframe\n",
    "cols = ['year', 'title', 'release_date', 'collection_name', 'sales', 'rolling_sales_collection']\n",
    "df_all = pd.merge(data_final_cal, df_collection[cols], how = 'left', \n",
    "                  on = ['year', 'title', 'release_date', 'collection_name', 'sales']).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A movie with bankable actors is more likely to have an important number of entrees than a movie with unknown \n",
    "# actors\n",
    "# We can leverage the \"cast\" feature that contains information about actors and their popularity for each movie\n",
    "# to create several features, for example:\n",
    "# - for one movie, compute the mean popularity of its 3 main actors\n",
    "# - for one movie, compute the mean popularity of its 5 main actors\n",
    "\n",
    "# In the 'cast' column we have the TMDB popularity associated to each actor present in the movie\n",
    "# We keep only the top 3 actors for each movie and we compute the average of their TMDB popularities\n",
    "# We apply a log function on the popularity in order to smooth the values and have a better distribution (otherwise\n",
    "# Jason Statham is largely #1...)\n",
    "df_all['mean_3_popularity'] = df_all['cast'].map(\n",
    "    lambda x: np.mean([np.log(el['tmdb_popularity']) if np.log(el['tmdb_popularity']) > 0 else 0 for el in x[:3]])) \\\n",
    "    .fillna(0)\n",
    "\n",
    "# We do the same for the top 5 actors\n",
    "df_all['mean_5_popularity'] = df_all['cast'].map(\n",
    "    lambda x: np.mean([np.log(el['tmdb_popularity']) if np.log(el['tmdb_popularity']) > 0 else 0 for el in x[:5]])) \\\n",
    "    .fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To go (much much) further:\n",
    "# In the same vein, we could create features taking into account sales of previous movies per actor and create \n",
    "# features that represent:\n",
    "# - for one movie, the mean of sales of previous movies of the #1 actor\n",
    "# - for one movie, the mean of sales of previous movies of the #2 actor\n",
    "# - for one movie, the mean of sales of previous movies of the #3 actor\n",
    "# - for one movie, the mean or the maximum of the three features above\n",
    "# This would also give an idea of an actor's \"popularity\"\n",
    "\n",
    "# /!\\ For more details, this process is presented in the main deck, at the end of the feature engineering part \n",
    "\n",
    "# We create three new columns with the names of the top 3 actors for each movie\n",
    "df_all['actor_1'] = df_all['cast'].map(lambda x: x[0]['name'] if len(x) > 0 else None) # name of actor #1\n",
    "df_all['actor_2'] = df_all['cast'].map(lambda x: x[1]['name'] if len(x) > 1 else None) # name of actor #2\n",
    "df_all['actor_3'] = df_all['cast'].map(lambda x: x[2]['name'] if len(x) > 2 else None) # name of actor #3\n",
    "# We define a list of all actors that appear in either #1, #2 or #3 positions for all movies\n",
    "actors_list = set(list(set(df_all['actor_1'])) + list(set(df_all['actor_2'])) + list(set(df_all['actor_3'])))\n",
    "\n",
    "k = 5\n",
    "df_all = df_all.sort_values('release_date')\n",
    "# For each actor we will compute the average of sales of its previous movies and we will copy this value to our\n",
    "# main dataframe when the given actor is in #1 or #2 or #3 position.\n",
    "for actor in list(actors_list):\n",
    "    # We find all the movies where a given actor is in either #1, #2 or #3 position\n",
    "    data_actor = df_all.loc[(df_all['actor_1'] == actor) | (df_all['actor_2'] == actor) | (df_all['actor_3'] == actor)]\n",
    "    data_actor['actor'] = actor\n",
    "    # We compute the average sales on its k previous movies (here k = 5)\n",
    "    data_actor['mean_sales'] = data_actor.groupby('actor')['sales'] \\\n",
    "        .transform(lambda x: x.rolling(k, 1).mean().shift()).fillna(0)\n",
    "    # We copy those values in the right place in our main dataframe\n",
    "    df_all.loc[df_all['actor_1'] == actor, 'actor_1'] = data_actor.loc[data_actor['actor_1'] == actor, 'mean_sales']\n",
    "    df_all.loc[df_all['actor_2'] == actor, 'actor_2'] = data_actor.loc[data_actor['actor_2'] == actor, 'mean_sales']\n",
    "    df_all.loc[df_all['actor_3'] == actor, 'actor_3'] = data_actor.loc[data_actor['actor_3'] == actor, 'mean_sales']\n",
    "\n",
    "# We rename the columns to make them more understandable\n",
    "df_all = df_all.rename({\n",
    "    'actor_1': 'actor_1_sales',\n",
    "    'actor_2': 'actor_2_sales',\n",
    "    'actor_3': 'actor_3_sales'\n",
    "}, axis=1)\n",
    "\n",
    "# We create two new features based on the ones created just above: the mean of the sales of the 3 main actors\n",
    "df_all['mean_sales_actor'] = (df_all['actor_1_sales'] + df_all['actor_2_sales'] + df_all['actor_3_sales']) / 3\n",
    "# and the maximum of the sales among the three main actors\n",
    "df_all['max_sales_actor'] = df_all[[\"actor_1_sales\", \"actor_2_sales\", \"actor_3_sales\"]].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop useless columns for modeling (not numerical columns, raw columns that have been transformed, ...)\n",
    "# Hint: you can use the .drop() method\n",
    "to_drop = ['nom_vacances', 'date', 'genres', 'production_countries', 'languages', 'is_adult', 'collection_name',\n",
    "           'overview', 'tagline', 'cast']\n",
    "df_all = df_all.drop(to_drop, axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values due to feature engineering if any\n",
    "# Hint: you can use the .fillna() method\n",
    "df_all = df_all.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "french_box_office (py37)",
   "language": "python",
   "name": "french_box_office"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
