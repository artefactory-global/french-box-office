{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://upload.wikimedia.org/wikipedia/fr/thumb/e/e5/Logo_%C3%A9cole_des_ponts_paristech.svg/676px-Logo_%C3%A9cole_des_ponts_paristech.svg.png\" width=\"200\"  height=\"200\" hspace=\"200\"/> </td>\n",
    "<td> <img src=\"https://pbs.twimg.com/profile_images/1156541928193896448/5ihYIbCQ_200x200.png\" width=\"200\" height=\"200\" /> </td>\n",
    "</tr></table>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<h1><center>Advanced Modelling</center></h1>\n",
    "\n",
    "\n",
    "\n",
    "<font size=\"3\">This session is divided into **2** main parts:\n",
    "- **1 - Classification example adapted to computer vision (Handwritten digit recognition)**\n",
    "- **2 - Regression example adapted to IMDB data**\n",
    "\n",
    "In each of these parts, some **guidelines** and **hints** are given for each task. \n",
    "Do not hesitate to check the links to documentation to understand the functions you use. \n",
    "    \n",
    "The goal of this session is for you to **be able to create a deep learning model for a regression purpose** (predicting french movies sales). We will use a **classification example adapted to computer vision first** in order for you to be familiar with the Deep Learning processes and we will try to replicate the word and create a fully connected network for our regression problem. All the work is based on **pytorch** and the computer vision example (MNIST) is inspired by a tutorial done by the fastai team.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Classification on MNIST dataset (handwritten digit recognition)\n",
    "#### *NB: Nothing to fill, just run the cells for this part*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Loading the MNIST dataset from pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "import pickle\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "        content = requests.get(URL + FILENAME).content\n",
    "        (PATH / FILENAME).open(\"wb\").write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The dataset comes already with a training, a validation and a test sets. We will use them as is.\n",
    "### The data is stored as numpy array\n",
    "\n",
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), (x_test, y_test)) = pickle.load(f, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Let's visualize an image \n",
    "pyplot.imshow(x_train[4].reshape((28, 28)), cmap=\"gray\")\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Before we proceed, we will transform the numpy to tensor as it is easier to work with for deep learning\n",
    "### [TO DO YAGUE]: explain tensor principle\n",
    "x_train, y_train, x_valid, y_valid, x_test, y_test = map(\n",
    "    torch.tensor, (x_train, y_train, x_valid, y_valid, x_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lets build a dataset using the TensorDataset of pytorch library\n",
    "### A dataset object is a convenient way to have x (features, here the image) and y (the label, which digit is it) \n",
    "### at the same time\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "test_ds = TensorDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### A dataloader is also convenient object for deep learning as we can iterate the dataset by batch\n",
    "### All you have to do is pick a batch size and by iterationg over the dataloader, you will receive batch of your dataset\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "bs= 64\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=bs*2)\n",
    "test_dl = DataLoader(test_ds, batch_size=bs*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "\n",
    "\n",
    "def preprocess(x):\n",
    "    return x.view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NN.Sequential allows us to easily stack layers to build an network of convolutionnal layers. It is usefull\n",
    "### here because we are dealing with images\n",
    "### The first layer (Lambda) is just here to transform the vector into a 28x28 pixel square image\n",
    "### ReLu is an activation function (more info in the appendix of the session 8 presentation)\n",
    "### AvgPool2d is an Average pooling layer (more info in the appendix of the session 8 presentation)\n",
    "\n",
    "# We initialize the model with the previously mentioned layers\n",
    "model = nn.Sequential(\n",
    "    Lambda(preprocess),\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(4),\n",
    "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "### We define the parameters of the network:\n",
    "\n",
    "### Choose a learning rate\n",
    "lr = 0.1\n",
    "\n",
    "### Define an optimizer to run our gradient descent algorithm. We choose a stochastic gradient descent (SGD). \n",
    "### NB: Momentum helps the training go faster but it is optional\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "### Let's define the loss function.\n",
    "### We pick the cross entropy loss as we are facing a classification problem\n",
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# We define a function to compute the loss for a batch (i.e. a sample) of data\n",
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    ''' Will compute the loss for a batch and the gradient if opt is given'''\n",
    "    loss = loss_func(model(xb), yb)\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "# We define a function to fit (i.e. train) the model on our data (train and valid datasets)\n",
    "# Different parameters must be precised: \n",
    "# - epochs (number of training iterations to do), \n",
    "# - model (neural network architecture), \n",
    "# - loss_func (loss function), \n",
    "# - opt (optimizer), \n",
    "# - train_dl (training dataset, dataloader object), \n",
    "# - valid_dl (validation dataset, dataloader object)\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        # We train the model\n",
    "        model.train()\n",
    "        # We iterate on the training data per batch\n",
    "        for xb, yb in train_dl:\n",
    "            # We compute the loss on each batch\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "        # We evaluate the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # We compute the loss per batch\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
    "            )\n",
    "        # We sum up the losses for the entire validation set\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "        # We print the loss for each epoch\n",
    "        print(epoch, val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Choose a number of epochs for training (less than 50 otherwise it will take a long time to run)\n",
    "epochs= 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.30515610204935073\n",
      "1 0.2977557823181152\n",
      "2 0.21001164155006408\n",
      "3 0.20034705535173417\n",
      "4 0.15707349194288253\n",
      "5 0.14662755954265594\n",
      "6 0.14445211496055127\n",
      "7 0.13919810348749162\n",
      "8 0.13430282634794713\n",
      "9 0.13992671927511693\n",
      "10 0.11888606450706721\n",
      "11 0.12575418030023575\n",
      "12 0.13745020753741263\n",
      "13 0.15738906062841415\n",
      "14 0.12075926521718502\n",
      "15 0.11851345114260912\n",
      "16 0.1229674611479044\n",
      "17 0.1266410844348371\n",
      "18 0.14309599875733256\n",
      "19 0.11794129944294691\n",
      "--- 3.25 minutes ---\n"
     ]
    }
   ],
   "source": [
    "### Call the 'fit' function to train the model using the dataloaders for train and valid and it will print the \n",
    "### loss for the validation set\n",
    "import time\n",
    "start_time = time.time()\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)\n",
    "print(\"--- %s minutes ---\" % np.round((time.time() - start_time)/60, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4 Prediction and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We make predictions on the validation set\n",
    "model.eval()\n",
    "predictions = model(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9652)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### We compute the accuracy of the classifier. \n",
    "### This is the metric that tells us how good we can recognize handwritten digits\n",
    "(torch.argmax(predictions, dim = 1) == y_test).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Regression on IMDB data (predicting french movies sales)\n",
    "#### *NB: This part has some cells that need to be filled*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x103382b70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Loading the IMDB dataset as in previous sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Loading data\n",
    "def load_dataset(path):\n",
    "    print(f\"loading raw data..\")\n",
    "    data = pd.read_csv(path)\n",
    "    data.drop(['title'], axis = 1, inplace = True)\n",
    "    return data\n",
    "\n",
    "# Cleaning data (remove 2020 year, ...)\n",
    "def clean_data(data, drop_2020=True):\n",
    "    print(f\"cleaning data..\")\n",
    "    data = data.dropna()\n",
    "    if drop_2020:\n",
    "        data = data.query(\"year != 2020\")\n",
    "    data = data.sort_values(by='release_date')\n",
    "    data.release_date = pd.to_datetime(data.release_date)\n",
    "    data.index = data.release_date\n",
    "    data = data.drop(columns = ['index', 'release_date', 'year'], errors='ignore')\n",
    "    return data\n",
    "\n",
    "# Split dataset in train, validation and test sets by date (not randomly because it is a time series)\n",
    "def train_test_split_by_date(df: pd.DataFrame, split_date_val: str, split_date_test: str):\n",
    "    \"\"\"Split dataset according to a split date in format \"YYYY-MM-DD\"\n",
    "    - train: [:split_date_1[\n",
    "    - validation: [split_date_1: split_date_2[\n",
    "    - test: [split_date_2:[\n",
    "    \"\"\"\n",
    "    train = df.loc[:split_date_val].copy()\n",
    "    validation = df.loc[split_date_val:split_date_test].copy()\n",
    "    test = df.loc[split_date_test:].copy()\n",
    "    return train, validation, test\n",
    "\n",
    "# Get the explanatory variables (features) and the target variable\n",
    "def get_x_y(dataset):\n",
    "    target = dataset.sales\n",
    "    target = target.astype(float)\n",
    "    features = dataset.drop(columns = ['sales'], errors='ignore')\n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading raw data..\n",
      "cleaning data..\n"
     ]
    }
   ],
   "source": [
    "# Update the path below with your own path to the dataset 'data_prepared_session4.csv' on your laptop:\n",
    "#path = '/Users/yaguethiam/Ponts/data_prepared_session4.csv'\n",
    "path = '../session4/data_prepared_session4.csv'\n",
    "\n",
    "# Load the dataset\n",
    "raw_data = load_dataset(path)\n",
    "# Clean the dataset\n",
    "data = clean_data(raw_data, drop_2020=False)\n",
    "# Split into train, validation and test sets\n",
    "train_data, validation_data, test_data = train_test_split_by_date(data,\n",
    "                                                                  '2018-01-01',\n",
    "                                                                  '2020-01-01')\n",
    "# Get features and target on each dataset\n",
    "train_x, train_y = get_x_y(train_data)\n",
    "validation_x, validation_y = get_x_y(validation_data)\n",
    "test_x, test_y = get_x_y(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Data preparation (transform into tensors then dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "### Create a TensorDataset for train and validation \n",
    "### /!\\ Don't forget to take the logarithm of the target variable y \n",
    "### Convert the torch to float \n",
    "\n",
    "x_train  = torch.tensor(train_x.values).float()\n",
    "x_valid  = torch.tensor(validation_x.values).float()\n",
    "x_test   = torch.tensor(test_x.values).float()\n",
    "\n",
    "y_train = torch.tensor(train_y.values).log().unsqueeze(1).float()\n",
    "y_valid = torch.tensor(validation_y.values).log().unsqueeze(1).float()\n",
    "y_test  = torch.tensor(test_y.values).log().unsqueeze(1).float()\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "test_ds = TensorDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "### Create a Dataloader from the tensors\n",
    "### Choose a batch size \n",
    "### Hint: 500 is a good choice but feel free to change it\n",
    "\n",
    "bs = 500\n",
    "train_dl = DataLoader(train_ds, batch_size = bs, shuffle = False)\n",
    "valid_dl = DataLoader(valid_ds, batch_size = bs)\n",
    "test_dl = DataLoader(test_ds, batch_size = bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the number of input features into the variable 'm'\n",
    "m = x_train.shape[1]\n",
    "\n",
    "### Choose a number of hidden layers\n",
    "nh = 600\n",
    "\n",
    "### Choose a value for the dropout probability. \n",
    "### Hint: 0.15 is a good choice\n",
    "dropout = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute the mean and the standard deviation of the training data\n",
    "### Why do we only compute the mean and standard deviation of the training data for the normalization to come?\n",
    "### [TO DO YAGUE]: answer?\n",
    "\n",
    "train_mean, train_std = x_train.mean(), x_train.std()\n",
    "\n",
    "def normalize(x, m=train_mean, s=train_std):\n",
    "    '''\n",
    "    Normalize a dataset x with the mean (m) and the std dev (s)\n",
    "    '''\n",
    "    return (x - m) / s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build a sequential model using several layers in the right order:\n",
    "### - Lambda (defined above, use the appropriate function in input, remember we want to normalize before \n",
    "###   anything happens)\n",
    "### - nn.Linear\n",
    "### - nn.Dropout (used for regularization)\n",
    "### - nn.ReLu (used to build a non-linear model)\n",
    "### - nn.Linear (output layer: should be of 1 dimension as we are doing a regression)\n",
    "### Organize these layers correctly below. \n",
    "\n",
    "model = nn.Sequential(\n",
    "    Lambda(normalize),\n",
    "    nn.Linear(m,nh),\n",
    "    nn.Dropout(dropout),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(nh,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Choose an optimizer which uses gradient descent\n",
    "### Give your optimizer the right parameters\n",
    "### Optional: add momentum to it (it will help accelarate training, set it to 0.9 for example)\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "lr = 0.2   # learning rate\n",
    "momentum = 0.9\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=momentum) # optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Choose a number of epochs. The goal here is a number of epoch enough for your model to train and to not \n",
    "### overfit. You can try many values (not more than 100 if you want the training to be \"fast\")\n",
    "\n",
    "epochs = 45 # how many epochs to train for\n",
    "\n",
    "### Choose a loss function, remember what we did in the last session (it will not be the same but you can \n",
    "### choose one that will be very similar)\n",
    "### Hint: for example L1Loss() but other loss are availabe, check the documentation for more info: \n",
    "### https://pytorch.org/docs/stable/nn.html#loss-functions\n",
    "loss_func = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# We define a function to compute the loss for a batch (i.e. a sample) of data\n",
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    ''' Will compute the loss for a batch and the gradient if opt is given'''\n",
    "    loss = loss_func(model(xb), yb)\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "# We define a function to fit (i.e. train) the model on our data (train and valid datasets)\n",
    "# Different parameters must be precised: \n",
    "# - epochs (number of training iterations to do), \n",
    "# - model (neural network architecture), \n",
    "# - loss_func (loss function), \n",
    "# - opt (optimizer), \n",
    "# - train_dl (training dataset, dataloader object), \n",
    "# - valid_dl (validation dataset, dataloader object)\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        # We train the model\n",
    "        model.train()\n",
    "        # We iterate on the training data per batch\n",
    "        for xb, yb in train_dl:\n",
    "            # We compute the loss on each batch\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "        # We evaluate the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # We compute the loss per batch\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
    "            )\n",
    "        # We sum up the losses for the entire validation set\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "        # We print the loss for each epoch\n",
    "        print(epoch, val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.593199077405427\n",
      "1 1.605095298666703\n",
      "2 1.2365806761540865\n",
      "3 1.1886418806879144\n",
      "4 1.2277353625548513\n",
      "5 1.1895310470932408\n",
      "6 1.1821189679597552\n",
      "7 1.2088692125521208\n",
      "8 1.1758493341897662\n",
      "9 1.1959153037322194\n",
      "10 1.1873749682777806\n",
      "11 1.1863461795606112\n",
      "12 1.18363524738111\n",
      "13 1.1899676605274803\n",
      "14 1.178733389628561\n",
      "15 1.1935930408929523\n",
      "16 1.196315834396764\n",
      "17 1.1890352587950856\n",
      "18 1.18516827257056\n",
      "19 1.183755055854195\n",
      "20 1.1880590005924827\n",
      "21 1.1775811314582825\n",
      "22 1.186477607802341\n",
      "23 1.1863717687757391\n",
      "24 1.1948193311691284\n",
      "25 1.178001996717955\n",
      "26 1.1775296424564563\n",
      "27 1.184769818657323\n",
      "28 1.1667539696944387\n",
      "29 1.1837405970222072\n",
      "30 1.1776097448248612\n",
      "31 1.1856837649094432\n",
      "32 1.1816329077670449\n",
      "33 1.1886571928074485\n",
      "34 1.2070227328099703\n",
      "35 1.186454923529374\n",
      "36 1.1783447987154911\n",
      "37 1.1789093143061589\n",
      "38 1.233493237118972\n",
      "39 1.193886769445319\n",
      "40 1.1786415388709621\n",
      "41 1.2002847100559033\n",
      "42 1.2694188575995595\n",
      "43 1.335924390115236\n",
      "44 1.3213552330669605\n",
      "--- 6.831168174743652 seconds ---\n"
     ]
    }
   ],
   "source": [
    "### Call the 'fit' function to train your model\n",
    "import time\n",
    "start_time = time.time()\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3214, grad_fn=<L1LossBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Compute the loss function on the validation set\n",
    "loss_func(model(x_valid), y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 Predictions and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    \"\"\"in percent\"\"\"\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred)/y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113.24774026870728"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Compute the new MAPE on the validation set\n",
    "mean_absolute_percentage_error(model(x_test).exp().detach().numpy(), y_test.exp().detach().numpy())\n",
    "\n",
    "### In comparison with what we obtained with classic algorithms in previous sessions, we can see that the MAPE\n",
    "### is better with DL models, even a very simple one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
