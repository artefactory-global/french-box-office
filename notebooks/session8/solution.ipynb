{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "interested-brazilian",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://upload.wikimedia.org/wikipedia/fr/thumb/e/e5/Logo_%C3%A9cole_des_ponts_paristech.svg/676px-Logo_%C3%A9cole_des_ponts_paristech.svg.png\" width=\"200\"  height=\"200\" hspace=\"200\"/> </td>\n",
    "<td> <img src=\"https://pbs.twimg.com/profile_images/1156541928193896448/5ihYIbCQ_200x200.png\" width=\"200\" height=\"200\" /> </td>\n",
    "</tr></table>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<h1><center>Advanced Modelling</center></h1>\n",
    "\n",
    "\n",
    "\n",
    "<font size=\"3\">This session is divided into **2** parts:\n",
    "- **1 - Example with computer vision: handwritten digit classifier**\n",
    "- **2 - Deep Learning with tabular data: french-box-office**\n",
    "\n",
    "In each of these parts, some **guidelines** and **hints** are given for each task. \n",
    "Do not hesitate to check the links to documentation to understand the functions you use. \n",
    "    \n",
    "The goal of this session is to create a regression using deep learning. We will use an example based on computer vision and we will try to create a fully connected network for our regression problem. All the work is based on pytorch and the computer vision is taken from a tutorial done by the fastai team.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-christopher",
   "metadata": {},
   "source": [
    "# Classification: handwritten digit recognition -- MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-liquid",
   "metadata": {},
   "source": [
    "#### Getting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "exciting-serum",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "import pickle\n",
    "\n",
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "        content = requests.get(URL + FILENAME).content\n",
    "        (PATH / FILENAME).open(\"wb\").write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "equivalent-jewelry",
   "metadata": {},
   "outputs": [],
   "source": [
    "### import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "conditional-snake",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "\n",
    "pyplot.imshow(x_train[4].reshape((28, 28)), cmap=\"gray\")\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dense-nothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "vocal-submission",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = map(\n",
    "    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "applied-tower",
   "metadata": {},
   "outputs": [],
   "source": [
    "### dataset\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "valid_ds = TensorDataset(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "supported-desert",
   "metadata": {},
   "outputs": [],
   "source": [
    "### dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "bs= 64\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "valid_dl = DataLoader(train_ds, batch_size=bs*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "looking-worst",
   "metadata": {},
   "outputs": [],
   "source": [
    "### model\n",
    "from torch import nn\n",
    "\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "\n",
    "\n",
    "def preprocess(x):\n",
    "    return x.view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sunset-metro",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    Lambda(preprocess),\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(4),\n",
    "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "future-reference",
   "metadata": {},
   "outputs": [],
   "source": [
    "### optimizer\n",
    "from torch import optim\n",
    "\n",
    "lr = 0.1  \n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "female-sleeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loss function\n",
    "import torch.nn.functional as F\n",
    "\n",
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "electric-breeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "### fit\n",
    "import numpy as np\n",
    "\n",
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
    "            )\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "\n",
    "        print(epoch, val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "catholic-impossible",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "collective-camel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.4759799555015564\n",
      "1 0.28878126936912535\n",
      "2 0.2655349987316132\n",
      "3 0.1844113236618042\n",
      "4 0.1703712854242325\n",
      "5 0.159769218416214\n",
      "6 0.13608565761566163\n",
      "7 0.13180594417095184\n",
      "8 0.13037776471853257\n",
      "9 0.11728199555397034\n"
     ]
    }
   ],
   "source": [
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "atmospheric-memorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = model(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "available-importance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9629)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.argmax(predictions,dim=1)==y_valid).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-theology",
   "metadata": {},
   "source": [
    "# Regression with Deep Learning -- French Box Office"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "threaded-canvas",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fef94216eb0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-wonder",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "through-applicant",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "def load_dataset(path):\n",
    "    print(f\"loading raw data..\")\n",
    "    data = pd.read_csv(path)\n",
    "    data.drop(['title'], axis = 1, inplace = True)\n",
    "    return data\n",
    "\n",
    "def clean_data(data, drop_2020=True):\n",
    "    print(f\"cleaning data..\")\n",
    "    data = data.dropna()\n",
    "    if drop_2020:\n",
    "        data = data.query(\"year != 2020\")\n",
    "    data = data.sort_values(by='release_date')\n",
    "    data.release_date = pd.to_datetime(data.release_date)\n",
    "    data.index = data.release_date\n",
    "    data = data.drop(columns = ['index', 'release_date', 'year'], errors='ignore')\n",
    "    return data\n",
    "\n",
    "def train_test_split_by_date(df: pd.DataFrame, split_date_val: str, split_date_test: str):\n",
    "    \"\"\"Split dataset according to a split date in format \"YYYY-MM-DD\"\n",
    "    - train: [:split_date_1[\n",
    "    - validation: [split_date_1: split_date_2[\n",
    "    - test: [split_date_2:[\n",
    "    \"\"\"\n",
    "    train = df.loc[:split_date_val].copy()\n",
    "    validation = df.loc[split_date_val:split_date_test].copy()\n",
    "    test = df.loc[split_date_test:].copy()\n",
    "    return train, validation, test\n",
    "\n",
    "def get_x_y(dataset):\n",
    "    target = dataset.sales\n",
    "    target = target.astype(float)\n",
    "    features = dataset.drop(columns = ['sales'], errors='ignore')\n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "selective-palestine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading raw data..\n",
      "cleaning data..\n"
     ]
    }
   ],
   "source": [
    "path = '/Users/yaguethiam/Ponts/data_prepared_session4.csv'\n",
    "raw_data = load_dataset(path)\n",
    "\n",
    "data = clean_data(raw_data, drop_2020=False)\n",
    "train_data, validation_data, test_data = train_test_split_by_date(data,\n",
    "                                                                  '2018-01-01',\n",
    "                                                                  '2020-01-01')\n",
    "train_x, train_y = get_x_y(train_data)\n",
    "validation_x, validation_y = get_x_y(validation_data)\n",
    "test_x, test_y = get_x_y(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "smooth-daughter",
   "metadata": {},
   "outputs": [],
   "source": [
    "### create a TensorDataset for train and validation \n",
    "### Dont forget to take the log of y \n",
    "### convert the torch to float \n",
    "x_train  = torch.tensor(train_x.values).float()\n",
    "x_valid  = torch.tensor(validation_x.values).float()\n",
    "\n",
    "y_train = torch.tensor(train_y.values).log().unsqueeze(1).float()\n",
    "y_valid = torch.tensor(validation_y.values).log().unsqueeze(1).float()\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "valid_ds = TensorDataset(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "satisfactory-daniel",
   "metadata": {},
   "outputs": [],
   "source": [
    "### create a Dataloader\n",
    "### choose a batch size, 500 is a good choice but feel free to change it\n",
    "\n",
    "bs = 500\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "valid_dl = DataLoader(train_ds, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "proper-israel",
   "metadata": {},
   "outputs": [],
   "source": [
    "### compute mean and std of the training data\n",
    "### why do we only compute the mean and std deviation of the training data for the normalization to come?\n",
    "\n",
    "train_mean,train_std = x_train.mean(),x_train.std()\n",
    "\n",
    "def normalize(x, m=train_mean, s=train_std):\n",
    "    '''\n",
    "    Normalize a dataset x with the mean m and the std dev s\n",
    "    '''\n",
    "    return (x-m)/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "infrared-damage",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = x_train.shape[1]\n",
    "\n",
    "### choose a number of hidden layers\n",
    "nh = 500\n",
    "\n",
    "### choose a value for the dropout probability. 0.15 is a good choice\n",
    "dropout = 0.15\n",
    "\n",
    "## add weight decay ??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "collective-providence",
   "metadata": {},
   "outputs": [],
   "source": [
    "### build a sequential model using Lambda (defined above, use the appropriate function)\n",
    "### Use also nn.Linear and nn.Dropout if you want to add regularization\n",
    "### nn.ReLu if you want to build a non-linear model\n",
    "### organize the layer correctly. Remember we want to normalize before anything happens\n",
    "### and the output layer should be of 1 dimension as we are doing a regression\n",
    "\n",
    "model = nn.Sequential(Lambda(normalize), nn.Linear(m,nh), nn.Dropout(dropout), nn.ReLU(),\n",
    "                     nn.Linear(nh,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "canadian-dryer",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Choose an optimizer which uses gradient descent\n",
    "### give your optimizer the right parameters\n",
    "### add momentum to it (it will help accelarate training, set it to 0.9)\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "lr = 0.09   # learning rate\n",
    "momentum = 0.9\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "final-syndicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Choose a number of epochs. The goal here is a number of epoch enough for your model to train and to not overfitt\n",
    "### you can try many values, the training is fast\n",
    "### choose a loss function, remember what we did in the last session (it will not be the same but you can choose one that will be very similar)\n",
    "\n",
    "epochs = 60 # how many epochs to train for\n",
    "loss_func = nn.L1Loss() #loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "binary-weekly",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.8237561622935279\n",
      "1 1.3785576535055564\n",
      "2 1.530764989342294\n",
      "3 1.522971267346449\n",
      "4 1.2302988436687068\n",
      "5 1.353371971043295\n",
      "6 1.2261012428436715\n",
      "7 1.2414786414277357\n",
      "8 1.1865867734875764\n",
      "9 1.2096693125616267\n",
      "10 1.4686641955623774\n",
      "11 1.3779221336967236\n",
      "12 1.2265548501169974\n",
      "13 1.2022442777671443\n",
      "14 1.215140857045064\n",
      "15 1.2294768259084317\n",
      "16 1.1975795918160854\n",
      "17 1.1980381019165869\n",
      "18 1.1670618292948431\n",
      "19 1.1734920474817865\n",
      "20 1.1640494459225212\n",
      "21 1.1588617761329583\n",
      "22 1.1638071597842972\n",
      "23 1.199037336510834\n",
      "24 1.1456658025042408\n",
      "25 1.15078948059511\n",
      "26 1.1664474648731433\n",
      "27 1.1601201537152432\n",
      "28 1.2201739802141245\n",
      "29 1.1521001851730757\n",
      "30 1.160767086362951\n",
      "31 1.1605069167104172\n",
      "32 1.1371022781376552\n",
      "33 1.172571165043862\n",
      "34 1.1548483213269098\n",
      "35 1.1399908928952789\n",
      "36 1.1444840636098093\n",
      "37 1.1446727285972575\n",
      "38 1.169626389225284\n",
      "39 1.1512216636901655\n",
      "40 1.1552409347410608\n",
      "41 1.1360509970637127\n",
      "42 1.2040603930376328\n",
      "43 1.182957324856838\n",
      "44 1.184087605274696\n",
      "45 1.1736622991558687\n",
      "46 1.1563851345541813\n",
      "47 1.1288934161135638\n",
      "48 1.173656610883935\n",
      "49 1.1660211303963202\n",
      "50 1.1332394400872252\n",
      "51 1.1727992340637239\n",
      "52 1.134430651858414\n",
      "53 1.158498194162459\n",
      "54 1.1307055106415929\n",
      "55 1.1386421740395225\n",
      "56 1.1352958534739488\n",
      "57 1.1209723475636473\n",
      "58 1.121312504508584\n",
      "59 1.1292548124403632\n"
     ]
    }
   ],
   "source": [
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "informed-fifty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1275, grad_fn=<L1LossBackward>)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### compute the loss function on the validation set\n",
    "loss_func(model(x_valid), y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "confidential-selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "parliamentary-bosnia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    \"\"\"in percent\"\"\"\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred)/y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "fleet-acrobat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112.6554012298584"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### compute the new MAPE on the validation set\n",
    "\n",
    "mean_absolute_percentage_error(model(x_valid).exp().detach().numpy(), y_valid.exp().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-shade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-yeast",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
