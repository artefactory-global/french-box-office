{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://upload.wikimedia.org/wikipedia/fr/thumb/e/e5/Logo_%C3%A9cole_des_ponts_paristech.svg/676px-Logo_%C3%A9cole_des_ponts_paristech.svg.png\" width=\"200\"  height=\"200\" hspace=\"200\"/> </td>\n",
    "<td> <img src=\"https://pbs.twimg.com/profile_images/1156541928193896448/5ihYIbCQ_200x200.png\" width=\"200\" height=\"200\" /> </td>\n",
    "</tr></table>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<h1><center>Session 6 - Unsupervised modeling</center></h1>\n",
    "\n",
    "\n",
    "\n",
    "<font size=\"3\">This session is divided into **3** parts:\n",
    "- **Loading data**\n",
    "- **Clustering**\n",
    "- **Topic modeling**\n",
    "\n",
    "In each of these parts, some **guidelines** and **hints** are given for each task. \n",
    "Do not hesitate to check the links to documentation to understand the functions you use. \n",
    "    \n",
    "The goal of this session is to **implement different unsupervised models** to **create clusters** among movies and to see **which topics emerge** from movies description.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - Useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rd\n",
    "import json\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset for unsupervised modeling\n",
    "data = pd.read_csv('dataset_unsupervised.csv')\n",
    "\n",
    "# Load tagline data\n",
    "with open('movies_tagline.json', 'r') as f:\n",
    "    tagline = json.load(f)\n",
    "df_tagline = pd.DataFrame.from_dict(tagline, orient='index', columns=['tagline']).reset_index().rename({'index': 'id'}, axis=1)\n",
    "df_tagline['id'] = df_tagline['id'].astype(int)\n",
    "\n",
    "# Load overview data\n",
    "with open('movies_overview.json', 'r') as f:\n",
    "    overview = json.load(f)\n",
    "df_overview = pd.DataFrame.from_dict(overview, orient='index', columns=['overview']).reset_index().rename({'index': 'id'}, axis=1)\n",
    "df_overview['id'] = df_overview['id'].astype(int)\n",
    "\n",
    "# Merge datasets\n",
    "data = data.merge(df_tagline, on='id', how='left').merge(df_overview, on='id', how='left')\n",
    "\n",
    "# Create dictionary to associate a movie ID to its title (will be used for exploration later)\n",
    "dict_title = data[['id', 'title']].set_index('id').to_dict()['title']\n",
    "\n",
    "# Drop variables useless for the modeling part (clustering and topic modeling)\n",
    "data = data.drop(['release_date', 'title', 'index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For movie clustering, textual variables are not usefull, let's drop them\n",
    "data_clustering = data.drop(['overview', 'tagline'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into a train part and a test part (with a ratio 80/20 for example)\n",
    "# Hint: take a look at the train_test_split() function from sklearn \n",
    "# (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_kmeans = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a number k of clusters \n",
    "k = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a K-means model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the K-means model to your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on your test set\n",
    "predictions = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore your results\n",
    "# Hint: merge your predictions with your initial dataset X_test_kmeans and use the dict_title object to retrieve \n",
    "# which title is associated to each movie ID. From there you will be able to know which movies are in each cluster\n",
    "X_test_kmeans['prediction'] = \n",
    "X_test_kmeans['title'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 10 movies (identified with their titles) associated to a each cluster\n",
    "# Hint: use a for loop on the number of clusters and use .loc to find movies that are related to the given cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the elbow method to find the optimal number k of clusters. Train a new K-means model with this number, make \n",
    "# predictions and explore the results\n",
    "# Hint: use the function plot_elbow() to determine the optimal k\n",
    "X_test_kmeans = X_test.copy()\n",
    "\n",
    "def plot_elbow(X_train, K):\n",
    "    Sum_of_squared_distances = []\n",
    "    for k in range(1, K):\n",
    "        km = KMeans(n_clusters = k, random_state = 0)\n",
    "        km = km.fit(X_train)\n",
    "        Sum_of_squared_distances.append(km.inertia_)\n",
    "    plt.plot(range(1, K), Sum_of_squared_distances, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Sum_of_squared_distances')\n",
    "    plt.title('Elbow Method For Optimal k')\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimal k and re-train a K-means model\n",
    "optimal_k = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set and add corresponding title\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 10 movies associated to a each cluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud\n",
    "from nlpretext import Preprocessor\n",
    "from nlpretext.basic.preprocess import (\n",
    "    fix_bad_unicode, lower_text, remove_eol_characters, remove_accents, remove_punct, remove_stopwords,\n",
    "    normalize_whitespace\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz\n",
    "#!pip install https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.3.0/fr_core_news_sm-2.3.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = ['vie', 'ans', 'jeune', 'film', 'femme', 'homme', 'famille', 'pere', 'fille', 'mere', 'monde',\n",
    "                    'jour', 'ete']\n",
    "\n",
    "def preprocess(df_text, custom_stopwords=custom_stopwords):\n",
    "    df_text = df_text.loc[df_text['text'] != ' ']\n",
    "    df_text['text_prepro'] = df_text['text'].map(fix_bad_unicode)\n",
    "    df_text['text_prepro'] = df_text['text_prepro'].map(lower_text)\n",
    "    df_text['text_prepro'] = df_text['text_prepro'].map(remove_eol_characters)\n",
    "    df_text['text_prepro'] = df_text['text_prepro'].map(remove_accents)\n",
    "    df_text['text_prepro'] = df_text['text_prepro'].map(remove_punct)\n",
    "    df_text['text_prepro'] = df_text['text_prepro'].map(lambda x: remove_stopwords(\n",
    "        x, lang='fr', custom_stopwords=custom_stopwords))\n",
    "    df_text['text_prepro'] = df_text['text_prepro'].map(normalize_whitespace)\n",
    "    df_text['tokens'] = df_text['text_prepro'].map(lambda x: x.split())\n",
    "    return df_text\n",
    "\n",
    "\n",
    "def make_word_cloud(text_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Function that computes word cloud from tokens\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text_df: pd.DataFrame\n",
    "        dataframe with text column\n",
    "    cmp: matplotlib.colors.LinearSegmentedColormap\n",
    "        colormap for the wordcloud\n",
    "    \"\"\"\n",
    "    text = ' '.join([' '.join(el) for el in text_df['tokens']])\n",
    "    if isinstance(text, str):\n",
    "        wordcloud_ = wordcloud.WordCloud(background_color='white', width=700, height=500).generate(text)\n",
    "    else:\n",
    "        raise TypeError('text_df contains non str values')\n",
    "    plt.imshow(wordcloud_)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 'text' column that is a combination of the tagline and the overview\n",
    "# Example:\n",
    "# - tagline = 'Il reprend du service.'\n",
    "# - overview = 'Arthur Bishop pensait qu'il avait mis son passé de tueur à gages derrière lui. ... etc'\n",
    "# -> text = 'Il reprend du service. Arthur Bishop pensait qu'il avait mis son passé de tueur à gages derrière lui. ... etc'\n",
    "data['text'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the results of the preprocessing\n",
    "preprocessed_text = preprocess(pd.DataFrame(data.iloc[0]).T)\n",
    "print('Raw text:', data['text'][0])\n",
    "print()\n",
    "print('Preprocessed text:', preprocessed_text['text_prepro'][0])\n",
    "print()\n",
    "print('Preprocessed tokens:', preprocessed_text['tokens'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Preprocess the 'text' column on the entire dataframe:\n",
    "# - transform the text to lowercase\n",
    "# - remove end of line characters\n",
    "# - remove accents\n",
    "# - remove punctuation\n",
    "# - remove stopwords\n",
    "# - split the preprocessed text into words (i.e. tokens) and store it into a new column named 'tokens'\n",
    "# Hint: use the 'preprocess' function defined above\n",
    "\n",
    "data = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all tokens associated to each movie and store it into the variable data_words\n",
    "# Hint: during the preprocessing, a new column 'tokens' has been created in the dataframe 'data'\n",
    "data_words = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dictionary based on the all the words per movie\n",
    "# Hint: explore the gensim library (specifically the 'corpora' section...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Corpus for each text associated to each movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a number of topics\n",
    "num_topics = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a LDA model\n",
    "# Hint: explore the gensim library (specifically the 'models' section...)\n",
    "lda_model = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the keywords in the n topics\n",
    "# Hint: maybe the gensim LDA model has a built-in function to do so..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# Visualize the topics\n",
    "# Hint: use pyLDAvis\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are your conclusions regarding the topics? (coherence, stability, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 To go further on topic modeling: Top2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from top2vec import Top2Vec\n",
    "# Pre-requisites for top2vec: keras_applications, keras_preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Topics interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained Top2Vec model\n",
    "model_bis = Top2Vec.load('top2vec_imdb.pickle')\n",
    "\n",
    "# Get information about the model: number of topics found and their sizes\n",
    "n_topics_found = model_bis.get_num_topics()\n",
    "topic_sizes, topic_nums = model_bis.get_topic_sizes()\n",
    "\n",
    "print('Number of topics found:', n_topics_found)\n",
    "print('Topic sizes:')\n",
    "for n in range(n_topics_found):\n",
    "    print('Topic ', n, ' - Size: ', topic_sizes[n])\n",
    "    print(' ----- ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the results part 1: check the texts with the best scores in each cluster\n",
    "\n",
    "# Change \"nb\" between 0 and 17 to explore each cluster\n",
    "# Clusters are sorted according to the number of texts they contain (exploring clusters between 0 and 10 may be\n",
    "# enough to have a good overview of what main clusters represent)\n",
    "nb = 0\n",
    "\n",
    "# You can also change the number of texts to display for each cluster with 'num_docs'\n",
    "num_docs = 7\n",
    "\n",
    "documents, document_scores, document_ids = model_bis.search_documents_by_topic(topic_num = nb, num_docs = num_docs)\n",
    "for doc, score, doc_id in zip(documents, document_scores, document_ids):\n",
    "    print(f\"Document: {doc_id}, Score: {score}\")\n",
    "    print(doc)\n",
    "    print(\"-----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the results part 2: check the wordclouds associated to each cluster\n",
    "# Wordclouds are really helpful to have a quick overview of most important words related to each topic\n",
    "# They can help to interpret each topic\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for nb in range(11):\n",
    "    documents, document_scores, document_ids = model_bis.search_documents_by_topic(topic_num=nb, num_docs=topic_sizes[nb])\n",
    "    df_text = preprocess(pd.DataFrame(documents, columns=['text']))\n",
    "    print('Topic', nb)\n",
    "    make_word_cloud(df_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What would be your interpretation for each cluster ?\n",
    "# Topic 0: \n",
    "# Topic 1: \n",
    "# Topic 2: \n",
    "# Topic 3: \n",
    "# Topic 4: \n",
    "# Topic 5: \n",
    "# Topic 6: \n",
    "# Topic 7: \n",
    "# Topic 8: \n",
    "# Topic 9: \n",
    "# Topic 10: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 To go further: train your own Top2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataset for top2vec model, from the 'data' dataframe, do not take movies with empty text \n",
    "# into account\n",
    "data_top2vec = \n",
    "\n",
    "# Make a list from the 'text' column that will contain all texts associated to all movies\n",
    "all_texts = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a top2vec model\n",
    "# Hint: use the \"speed\" argument to make the training faster\n",
    "model = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the results part: \n",
    "# - number of topics found\n",
    "# - topics sizes\n",
    "# Hint: check the documentation about top2vec to see what are the attributes of the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your model for later if you want to explore it in more details\n",
    "# model.save('top2vec_imdb_bis.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
