{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://upload.wikimedia.org/wikipedia/fr/thumb/e/e5/Logo_%C3%A9cole_des_ponts_paristech.svg/676px-Logo_%C3%A9cole_des_ponts_paristech.svg.png\" width=\"200\"  height=\"200\" hspace=\"200\"/> </td>\n",
    "<td> <img src=\"https://pbs.twimg.com/profile_images/1156541928193896448/5ihYIbCQ_200x200.png\" width=\"200\" height=\"200\" /> </td>\n",
    "</tr></table>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<h1><center>Session 6 - Unsupervised modeling</center></h1>\n",
    "\n",
    "\n",
    "\n",
    "<font size=\"3\">This session is divided into **3** parts:\n",
    "- **Loading data**\n",
    "- **Clustering**\n",
    "- **Topic modeling**\n",
    "\n",
    "In each of these parts, some **guidelines** and **hints** are given for each task. \n",
    "Do not hesitate to check the links to documentation to understand the functions you use. \n",
    "    \n",
    "The goal of this session is to **implement different unsupervised models** to **create clusters** among movies and to see **which topics emerge** from movies description.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - Useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rd\n",
    "import json\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset for unsupervised modeling\n",
    "data = pd.read_csv('dataset_unsupervised.csv')\n",
    "\n",
    "# Load tagline data\n",
    "with open('movies_tagline.json', 'r') as f:\n",
    "    tagline = json.load(f)\n",
    "df_tagline = pd.DataFrame.from_dict(tagline, orient='index', columns=['tagline']).reset_index().rename({'index': 'id'}, axis=1)\n",
    "df_tagline['id'] = df_tagline['id'].astype(int)\n",
    "\n",
    "# Load overview data\n",
    "with open('movies_overview.json', 'r') as f:\n",
    "    overview = json.load(f)\n",
    "df_overview = pd.DataFrame.from_dict(overview, orient='index', columns=['overview']).reset_index().rename({'index': 'id'}, axis=1)\n",
    "df_overview['id'] = df_overview['id'].astype(int)\n",
    "\n",
    "# Merge datasets\n",
    "data = data.merge(df_tagline, on='id', how='left').merge(df_overview, on='id', how='left')\n",
    "\n",
    "# Create dictionary to associate a movie ID to its title (will be used for exploration later)\n",
    "dict_title = data[['id', 'title']].set_index('id').to_dict()['title']\n",
    "\n",
    "# Drop variables useless for the modeling part (clustering and topic modeling)\n",
    "data = data.drop(['release_date', 'title', 'index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>sales</th>\n",
       "      <th>is_part_of_collection</th>\n",
       "      <th>budget</th>\n",
       "      <th>runtime</th>\n",
       "      <th>original_lang_en</th>\n",
       "      <th>original_lang_es</th>\n",
       "      <th>original_lang_fr</th>\n",
       "      <th>original_lang_it</th>\n",
       "      <th>original_lang_ja</th>\n",
       "      <th>...</th>\n",
       "      <th>rolling_sales_collection</th>\n",
       "      <th>mean_3_popularity</th>\n",
       "      <th>mean_5_popularity</th>\n",
       "      <th>actor_1_sales</th>\n",
       "      <th>actor_2_sales</th>\n",
       "      <th>actor_3_sales</th>\n",
       "      <th>mean_sales_actor</th>\n",
       "      <th>max_sales_actor</th>\n",
       "      <th>tagline</th>\n",
       "      <th>overview</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2475</th>\n",
       "      <td>2000</td>\n",
       "      <td>139087</td>\n",
       "      <td>0</td>\n",
       "      <td>25000000.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.489158</td>\n",
       "      <td>0.373572</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>Comme les Mousquetaires dont elles possèdent l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2870</th>\n",
       "      <td>2000</td>\n",
       "      <td>66228</td>\n",
       "      <td>0</td>\n",
       "      <td>22000000.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.404085</td>\n",
       "      <td>1.229533</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>New York, été 1977. Alors que la ville connait...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2706</th>\n",
       "      <td>2000</td>\n",
       "      <td>1463152</td>\n",
       "      <td>0</td>\n",
       "      <td>25000000.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11980</th>\n",
       "      <td>2000</td>\n",
       "      <td>32954</td>\n",
       "      <td>0</td>\n",
       "      <td>25000000.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.465217</td>\n",
       "      <td>0.346425</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>Félicia, dix-sept ans, traverse la mer d'Irlan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2480</th>\n",
       "      <td>2000</td>\n",
       "      <td>223564</td>\n",
       "      <td>1</td>\n",
       "      <td>40000000.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.802817</td>\n",
       "      <td>2.044138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Il reprend du service.</td>\n",
       "      <td>Arthur Bishop pensait qu'il avait mis son pass...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       year    sales  is_part_of_collection      budget  runtime  \\\n",
       "id                                                                 \n",
       "2475   2000   139087                      0  25000000.0    120.0   \n",
       "2870   2000    66228                      0  22000000.0    142.0   \n",
       "2706   2000  1463152                      0  25000000.0     77.0   \n",
       "11980  2000    32954                      0  25000000.0    116.0   \n",
       "2480   2000   223564                      1  40000000.0     99.0   \n",
       "\n",
       "       original_lang_en  original_lang_es  original_lang_fr  original_lang_it  \\\n",
       "id                                                                              \n",
       "2475                  0                 0                 1                 0   \n",
       "2870                  1                 0                 0                 0   \n",
       "2706                  0                 1                 0                 0   \n",
       "11980                 1                 0                 0                 0   \n",
       "2480                  1                 0                 0                 0   \n",
       "\n",
       "       original_lang_ja  ...  rolling_sales_collection  mean_3_popularity  \\\n",
       "id                       ...                                                \n",
       "2475                  0  ...                       0.0           0.489158   \n",
       "2870                  0  ...                       0.0           1.404085   \n",
       "2706                  0  ...                       0.0           0.000000   \n",
       "11980                 0  ...                       0.0           0.465217   \n",
       "2480                  0  ...                       0.0           2.802817   \n",
       "\n",
       "       mean_5_popularity  actor_1_sales  actor_2_sales  actor_3_sales  \\\n",
       "id                                                                      \n",
       "2475            0.373572            0.0            0.0            0.0   \n",
       "2870            1.229533            0.0            0.0            0.0   \n",
       "2706            0.000000            0.0            0.0            0.0   \n",
       "11980           0.346425            0.0            0.0            0.0   \n",
       "2480            2.044138            0.0            0.0            0.0   \n",
       "\n",
       "       mean_sales_actor  max_sales_actor                 tagline  \\\n",
       "id                                                                 \n",
       "2475                0.0              0.0                           \n",
       "2870                0.0              0.0                           \n",
       "2706                0.0              0.0                           \n",
       "11980               0.0              0.0                           \n",
       "2480                0.0              0.0  Il reprend du service.   \n",
       "\n",
       "                                                overview  \n",
       "id                                                        \n",
       "2475   Comme les Mousquetaires dont elles possèdent l...  \n",
       "2870   New York, été 1977. Alors que la ville connait...  \n",
       "2706                                                      \n",
       "11980  Félicia, dix-sept ans, traverse la mer d'Irlan...  \n",
       "2480   Arthur Bishop pensait qu'il avait mis son pass...  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For movie clustering, textual variables are not usefull, let's drop them\n",
    "data_clustering = data.drop(['overview', 'tagline'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick overview of the dataset for clustering\n",
    "data_clustering.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "cols_to_normalize = ['sales', 'budget', 'year', 'runtime', 'holidays', 'cos_month', 'nb_movie_collection', 'rolling_sales_collection',\n",
    "                     'mean_3_popularity', 'mean_5_popularity', 'actor_1_sales', 'actor_2_sales', 'actor_3_sales', 'mean_sales_actor',\n",
    "                     'max_sales_actor']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data_clustering[cols_to_normalize]) \n",
    "data_clustering[cols_to_normalize] = pd.DataFrame(scaled_data)\n",
    "data_clustering = data_clustering.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check results of normalization\n",
    "data_clustering.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into a train part and a test part (with a ratio 80/20 for example)\n",
    "# Hint: take a look at the train_test_split() function from sklearn \n",
    "# (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_kmeans = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a number k of clusters \n",
    "k = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a K-means model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the K-means model to your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on your test set\n",
    "predictions = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore your results\n",
    "# Hint: merge your predictions with your initial dataset X_test_kmeans and use the dict_title object to retrieve \n",
    "# which title is associated to each movie ID. From there you will be able to know which movies are in each cluster\n",
    "X_test_kmeans['id'] = X_test_kmeans.index.astype(int)\n",
    "X_test_kmeans['prediction'] = \n",
    "X_test_kmeans['title'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 10 movies (identified with their titles) associated to a each cluster\n",
    "# Hint: use a for loop on the number of clusters and use .loc to find movies that are related to the given cluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the elbow method to find the optimal number k of clusters. Train a new K-means model with this number, make \n",
    "# predictions and explore the results\n",
    "# Hint: use the function plot_elbow() to determine the optimal k\n",
    "X_test_kmeans = X_test.copy()\n",
    "\n",
    "def plot_elbow(X_train, K):\n",
    "    Sum_of_squared_distances = []\n",
    "    for k in range(1, K):\n",
    "        km = KMeans(n_clusters = k, random_state = 0)\n",
    "        km = km.fit(X_train)\n",
    "        Sum_of_squared_distances.append(km.inertia_)\n",
    "    plt.plot(range(1, K), Sum_of_squared_distances, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Sum_of_squared_distances')\n",
    "    plt.title('Elbow Method For Optimal k')\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimal k and re-train a K-means model\n",
    "optimal_k = \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set and add corresponding title\n",
    "X_test_kmeans = X_test.copy()\n",
    "predictions = \n",
    "\n",
    "# Append predictions and title to the test set\n",
    "X_test_kmeans['id'] = X_test_kmeans.index.astype(int)\n",
    "X_test_kmeans['prediction'] = \n",
    "X_test_kmeans['title'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 10 movies associated to a each cluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud\n",
    "from nlpretext import Preprocessor\n",
    "from nlpretext.basic.preprocess import (\n",
    "    fix_bad_unicode, lower_text, remove_eol_characters, remove_accents, remove_punct, remove_stopwords,\n",
    "    normalize_whitespace\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz\n",
    "#!pip install https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.3.0/fr_core_news_sm-2.3.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset ID as a feature and create regular index for data (from 0 to length of data)\n",
    "data['id'] = data.index.astype(int)\n",
    "data.index = range(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = ['vie', 'ans', 'jeune', 'film', 'femme', 'homme', 'famille', 'pere', 'fille', 'mere', 'monde',\n",
    "                    'jour', 'ete']\n",
    "\n",
    "def preprocess(df_text, custom_stopwords=custom_stopwords):\n",
    "    df_text = df_text.loc[df_text['text'] != ' ']\n",
    "    df_text['text_prepro'] = df_text['text'].map(fix_bad_unicode)\n",
    "    df_text['text_prepro'] = df_text['text_prepro'].map(lower_text)\n",
    "    df_text['text_prepro'] = df_text['text_prepro'].map(remove_eol_characters)\n",
    "    df_text['text_prepro'] = df_text['text_prepro'].map(remove_accents)\n",
    "    df_text['text_prepro'] = df_text['text_prepro'].map(remove_punct)\n",
    "    df_text['text_prepro'] = df_text['text_prepro'].map(lambda x: remove_stopwords(\n",
    "        x, lang='fr', custom_stopwords=custom_stopwords))\n",
    "    df_text['text_prepro'] = df_text['text_prepro'].map(normalize_whitespace)\n",
    "    df_text['tokens'] = df_text['text_prepro'].map(lambda x: x.split())\n",
    "    return df_text\n",
    "\n",
    "\n",
    "def make_word_cloud(text_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Function that computes word cloud from tokens\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text_df: pd.DataFrame\n",
    "        dataframe with text column\n",
    "    cmp: matplotlib.colors.LinearSegmentedColormap\n",
    "        colormap for the wordcloud\n",
    "    \"\"\"\n",
    "    text = ' '.join([' '.join(el) for el in text_df['tokens']])\n",
    "    if isinstance(text, str):\n",
    "        wordcloud_ = wordcloud.WordCloud(background_color='white', width=700, height=500).generate(text)\n",
    "    else:\n",
    "        raise TypeError('text_df contains non str values')\n",
    "    plt.imshow(wordcloud_)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------------------- To run only if nlpretext installation has failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-requisites:\n",
    "#!pip install -U pip setuptools wheel\n",
    "#!pip install -U spacy\n",
    "#!pip install https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.3.0/fr_core_news_sm-2.3.0.tar.gz\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "custom_stopwords = ['vie', 'ans', 'jeune', 'film', 'femme', 'homme', 'famille', 'pere', 'fille', 'mere', 'monde',\n",
    "                    'jour', 'ete']\n",
    "\n",
    "def preprocess_no_nlpretext(df_text, custom_stopwords=custom_stopwords):\n",
    "    df_text = df_text.loc[df_text['text'] != ' ']\n",
    "    df_text['text_prepro'] = df_text['text'].map(lambda x: x.lower())\n",
    "    df_text['text_prepro'] = df_text['text_prepro'].map(lambda x: x.replace('\\n', ''))\n",
    "    df_text['text_prepro'] = df_text['text_prepro'].map(lambda x: x.replace(\"’\", ' '))\n",
    "    df_text['text_prepro'] = df_text['text_prepro'].map(lambda x: re.sub(r'[^\\w\\s]','', x))\n",
    "    df_text['text_prepro'] = df_text['text_prepro'].map(lambda x: remove_stopwords_no_nlpretext(x, custom_stopwords=custom_stopwords))\n",
    "    df_text['text_prepro'] = df_text['text_prepro'].map(normalize_whitespace_no_nlpretext)\n",
    "    df_text['tokens'] = df_text['text_prepro'].map(lambda x: x.split())\n",
    "    return df_text\n",
    "\n",
    "def remove_stopwords_no_nlpretext(text, custom_stopwords):\n",
    "    sp = spacy.load('fr_core_news_sm')\n",
    "    stopwords = list(sp.Defaults.stop_words)\n",
    "    all_stopwords = stopwords + custom_stopwords + [\"l\", \"qu\", \"peutêtre\"]\n",
    "    return ' '.join([el for el in text.split() if el not in all_stopwords])\n",
    "\n",
    "def normalize_whitespace_no_nlpretext(text):\n",
    "    LINEBREAK_REGEX = re.compile(r\"((\\r\\n)|[\\n\\v])+\")\n",
    "    NONBREAKING_SPACE_REGEX = re.compile(r\"(?!\\n)\\s+\")\n",
    "    text = NONBREAKING_SPACE_REGEX.sub(\" \", LINEBREAK_REGEX.sub(r\"\\n\", text)).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 'text' column that is a combination of the tagline and the overview\n",
    "# Example:\n",
    "# - tagline = 'Il reprend du service.'\n",
    "# - overview = 'Arthur Bishop pensait qu'il avait mis son passé de tueur à gages derrière lui. ... etc'\n",
    "# -> text = 'Il reprend du service. Arthur Bishop pensait qu'il avait mis son passé de tueur à gages derrière lui. ... etc'\n",
    "data['text'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the results of the preprocessing\n",
    "# Feel free to change the value of 'i' to see several examples\n",
    "i = 0\n",
    "\n",
    "preprocessed_text = preprocess(pd.DataFrame(data.iloc[i]).T)\n",
    "print('Raw text:', data['text'][i])\n",
    "print()\n",
    "print('Preprocessed text:', preprocessed_text['text_prepro'][i])\n",
    "print()\n",
    "print('Preprocessed tokens:', preprocessed_text['tokens'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Preprocess the 'text' column on the entire dataframe:\n",
    "# - transform the text to lowercase\n",
    "# - remove end of line characters\n",
    "# - remove accents\n",
    "# - remove punctuation\n",
    "# - remove stopwords\n",
    "# - split the preprocessed text into words (i.e. tokens) and store it into a new column named 'tokens'\n",
    "# Hint: use the 'preprocess' function defined above\n",
    "\n",
    "data = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all tokens associated to each movie and store it into the variable data_words\n",
    "# Hint: during the preprocessing, a new column 'tokens' has been created in the dataframe 'data'\n",
    "data_words = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dictionary based on the all the words per movie\n",
    "# Hint: explore the gensim library (specifically the 'corpora' section...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Corpus for each text associated to each movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a number of topics\n",
    "num_topics = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a LDA model\n",
    "# Hint: explore the gensim library (specifically the 'models' section...)\n",
    "lda_model = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the keywords in the n topics\n",
    "# Hint: maybe the gensim LDA model has a built-in function to do so..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# Visualize the topics\n",
    "# Hint: use pyLDAvis\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are your conclusions regarding the topics? (coherence, stability, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 To go further on topic modeling: Top2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from top2vec import Top2Vec\n",
    "# Pre-requisites for top2vec: keras_applications, keras_preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Topics interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained Top2Vec model\n",
    "model_bis = Top2Vec.load('top2vec_imdb.pickle')\n",
    "\n",
    "# Get information about the model: number of topics found and their sizes\n",
    "n_topics_found = model_bis.get_num_topics()\n",
    "topic_sizes, topic_nums = model_bis.get_topic_sizes()\n",
    "\n",
    "print('Number of topics found:', n_topics_found)\n",
    "print('Topic sizes:')\n",
    "for n in range(n_topics_found):\n",
    "    print('Topic ', n, ' - Size: ', topic_sizes[n])\n",
    "    print(' ----- ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the results part 1: check the texts with the best scores in each cluster\n",
    "\n",
    "# Change \"nb\" between 0 and 17 to explore each cluster\n",
    "# Clusters are sorted according to the number of texts they contain (exploring clusters between 0 and 10 may be\n",
    "# enough to have a good overview of what main clusters represent)\n",
    "nb = 0\n",
    "\n",
    "# You can also change the number of texts to display for each cluster with 'num_docs'\n",
    "num_docs = 7\n",
    "\n",
    "documents, document_scores, document_ids = model_bis.search_documents_by_topic(topic_num = nb, num_docs = num_docs)\n",
    "for doc, score, doc_id in zip(documents, document_scores, document_ids):\n",
    "    print(f\"Document: {doc_id}, Score: {score}\")\n",
    "    print(doc)\n",
    "    print(\"-----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the results part 2: check the wordclouds associated to each cluster\n",
    "# Wordclouds are really helpful to have a quick overview of most important words related to each topic\n",
    "# They can help to interpret each topic\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for nb in range(11):\n",
    "    documents, document_scores, document_ids = model_bis.search_documents_by_topic(topic_num=nb, num_docs=topic_sizes[nb])\n",
    "    df_text = preprocess(pd.DataFrame(documents, columns=['text']))\n",
    "    print('Topic', nb)\n",
    "    make_word_cloud(df_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What would be your interpretation for each cluster ?\n",
    "# Topic 0: \n",
    "# Topic 1: \n",
    "# Topic 2: \n",
    "# Topic 3: \n",
    "# Topic 4: \n",
    "# Topic 5: \n",
    "# Topic 6: \n",
    "# Topic 7: \n",
    "# Topic 8: \n",
    "# Topic 9: \n",
    "# Topic 10: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 To go further: train your own Top2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataset for top2vec model, from the 'data' dataframe, do not take movies with empty text \n",
    "# into account\n",
    "data_top2vec = \n",
    "\n",
    "# Make a list from the 'text' column that will contain all texts associated to all movies\n",
    "all_texts = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a top2vec model\n",
    "# Hint: use the \"speed\" argument to make the training faster\n",
    "model = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the results part: \n",
    "# - number of topics found\n",
    "# - topics sizes\n",
    "# Hint: check the documentation about top2vec to see what are the attributes of the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your model for later if you want to explore it in more details\n",
    "# model.save('top2vec_imdb_bis.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
